{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import librosa,librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\BTP\n"
     ]
    }
   ],
   "source": [
    "cd F:\\BTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed silences from the audio files with an application.\n",
    "\n",
    "\"bef_rem\" is the original dataset of audiofiles.\n",
    "\n",
    "\"aft_rem\" is the dataset of audio files after the silence is removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bef_rem=os.listdir('DataSets/wav')\n",
    "aft_rem=os.listdir('remov/wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the silence is removed from the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x19462c61a20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_path_bef = 'Datasets/wav/08a02Na.wav'\n",
    "x , sr = librosa.load(audio_path_bef)\n",
    "#plt.axis([0, 3, -1, 1])\n",
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "plt.subplot(121)\n",
    "librosa.display.waveplot(x, sr=sr)\n",
    "\n",
    "audio_path_aft= 'remov/wav/08a02Na.wav'\n",
    "x , sr = librosa.load(audio_path_aft)\n",
    "#plt.axis([0, 3, -1, 1])\n",
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "plt.subplot(122)\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see form the above plots that the silence is removed within the audiofile.\n",
    "\n",
    "Let's extract mfcc values from each audio file and its emotion and store them.\n",
    "\n",
    "_x stores mfccs and _y stores emotions from each audio files in before and after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_size = 40\n",
    "bef_x = np.zeros((len(bef_rem),mfcc_size))\n",
    "bef_y = np.zeros((len(bef_rem),7),dtype = int)\n",
    "#bef_x stores the audio files' mfcc\n",
    "#bef_y stores emotions\n",
    "\n",
    "code = {\n",
    "    'W':0, #anger\n",
    "    'L':1, #boredom\n",
    "    'E':2, #disgust\n",
    "    'A':3, #fear\n",
    "    'F':4, #happy\n",
    "    'T':5, #sad\n",
    "    'N':6  #neutral\n",
    "}\n",
    "for i in range(len(bef_rem)):\n",
    "    X, sample_rate = librosa.load('DataSets/wav/'+bef_rem[i], res_type='kaiser_fast')\n",
    "    bef_y[i][code[bef_rem[i][5]]] = 1  \n",
    "    bef_x[i] = np.resize(np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=mfcc_size).T,axis=0),(1,mfcc_size))\n",
    "\n",
    "aft_x = np.zeros((len(aft_rem),mfcc_size))\n",
    "aft_y = np.zeros((len(aft_rem),7),dtype = int)\n",
    "\n",
    "#aft_x stores the mfcc after the removal of silences\n",
    "#aft_y stores emotions\n",
    "\n",
    "for i in range(len(aft_rem)):\n",
    "    X, sample_rate = librosa.load('remov/wav/'+aft_rem[i], res_type='kaiser_fast')\n",
    "    aft_y[i][code[aft_rem[i][5]]] = 1\n",
    "    aft_x[i] = np.resize(np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=mfcc_size).T,axis=0),(1,mfcc_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, categorize the datasets of respective emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_indices = list(np.where(bef_y[:,0]==1)[0])\n",
    "bef_angry_x = np.zeros((len(angry_indices),mfcc_size))\n",
    "bef_angry_y = np.zeros((len(angry_indices),7),dtype=int)\n",
    "aft_angry_x = np.zeros((len(angry_indices),mfcc_size))\n",
    "aft_angry_y = np.zeros((len(angry_indices),7),dtype=int)\n",
    "for i in range(len(angry_indices)):\n",
    "    bef_angry_x[i] = np.resize(bef_x[angry_indices[i]],(1,mfcc_size))\n",
    "    bef_angry_y[i] = np.resize(bef_y[angry_indices[i]],(1,7))\n",
    "    aft_angry_x[i] = np.resize(aft_x[angry_indices[i]],(1,mfcc_size))\n",
    "    aft_angry_y[i] = np.resize(aft_y[angry_indices[i]],(1,7))\n",
    "\n",
    "boredom_indices = list(np.where(bef_y[:,1]==1)[0])\n",
    "bef_boredom_x = np.zeros((len(boredom_indices),mfcc_size))\n",
    "bef_boredom_y = np.zeros((len(boredom_indices),7),dtype=int)\n",
    "aft_boredom_x = np.zeros((len(boredom_indices),mfcc_size))\n",
    "aft_boredom_y = np.zeros((len(boredom_indices),7),dtype=int)\n",
    "for i in range(len(boredom_indices)):\n",
    "    bef_boredom_x[i] = np.resize(bef_x[boredom_indices[i]],(1,mfcc_size))\n",
    "    bef_boredom_y[i] = np.resize(bef_y[boredom_indices[i]],(1,7))\n",
    "    aft_boredom_x[i] = np.resize(aft_x[boredom_indices[i]],(1,mfcc_size))\n",
    "    aft_boredom_y[i] = np.resize(aft_y[boredom_indices[i]],(1,7))\n",
    "\n",
    "disgust_indices = list(np.where(bef_y[:,2]==1)[0])\n",
    "bef_disgust_x = np.zeros((len(disgust_indices),mfcc_size))\n",
    "bef_disgust_y = np.zeros((len(disgust_indices),7),dtype=int)\n",
    "aft_disgust_x = np.zeros((len(disgust_indices),mfcc_size))\n",
    "aft_disgust_y = np.zeros((len(disgust_indices),7),dtype=int)\n",
    "for i in range(len(disgust_indices)):\n",
    "    bef_disgust_x[i] = np.resize(bef_x[disgust_indices[i]],(1,mfcc_size))\n",
    "    bef_disgust_y[i] = np.resize(bef_y[disgust_indices[i]],(1,7))\n",
    "    aft_disgust_x[i] = np.resize(aft_x[disgust_indices[i]],(1,mfcc_size))\n",
    "    aft_disgust_y[i] = np.resize(aft_y[disgust_indices[i]],(1,7))\n",
    "\n",
    "fear_indices = list(np.where(bef_y[:,3]==1)[0])\n",
    "bef_fear_x = np.zeros((len(fear_indices),mfcc_size))\n",
    "bef_fear_y = np.zeros((len(fear_indices),7),dtype=int)\n",
    "aft_fear_x = np.zeros((len(fear_indices),mfcc_size))\n",
    "aft_fear_y = np.zeros((len(fear_indices),7),dtype=int)\n",
    "for i in range(len(fear_indices)):\n",
    "    bef_fear_x[i] = np.resize(bef_x[fear_indices[i]],(1,mfcc_size))\n",
    "    bef_fear_y[i] = np.resize(bef_y[fear_indices[i]],(1,7))\n",
    "    aft_fear_x[i] = np.resize(aft_x[fear_indices[i]],(1,mfcc_size))\n",
    "    aft_fear_y[i] = np.resize(aft_y[fear_indices[i]],(1,7))\n",
    "\n",
    "happy_indices = list(np.where(bef_y[:,4]==1)[0])\n",
    "bef_happy_x = np.zeros((len(happy_indices),mfcc_size))\n",
    "bef_happy_y = np.zeros((len(happy_indices),7),dtype=int)\n",
    "aft_happy_x = np.zeros((len(happy_indices),mfcc_size))\n",
    "aft_happy_y = np.zeros((len(happy_indices),7),dtype=int)\n",
    "for i in range(len(happy_indices)):\n",
    "    bef_happy_x[i] = np.resize(bef_x[happy_indices[i]],(1,mfcc_size))\n",
    "    bef_happy_y[i] = np.resize(bef_y[happy_indices[i]],(1,7))\n",
    "    aft_happy_x[i] = np.resize(aft_x[happy_indices[i]],(1,mfcc_size))\n",
    "    aft_happy_y[i] = np.resize(aft_y[happy_indices[i]],(1,7))\n",
    "\n",
    "sad_indices = list(np.where(bef_y[:,5]==1)[0])\n",
    "bef_sad_x = np.zeros((len(sad_indices),mfcc_size))\n",
    "bef_sad_y = np.zeros((len(sad_indices),7),dtype=int)\n",
    "aft_sad_x = np.zeros((len(sad_indices),mfcc_size))\n",
    "aft_sad_y = np.zeros((len(sad_indices),7),dtype=int)\n",
    "for i in range(len(sad_indices)):\n",
    "    bef_sad_x[i] = np.resize(bef_x[sad_indices[i]],(1,mfcc_size))\n",
    "    bef_sad_y[i] = np.resize(bef_y[sad_indices[i]],(1,7))\n",
    "    aft_sad_x[i] = np.resize(aft_x[sad_indices[i]],(1,mfcc_size))\n",
    "    aft_sad_y[i] = np.resize(aft_y[sad_indices[i]],(1,7))\n",
    "\n",
    "neutral_indices = list(np.where(bef_y[:,6]==1)[0])\n",
    "bef_neutral_x = np.zeros((len(neutral_indices),mfcc_size))\n",
    "bef_neutral_y = np.zeros((len(neutral_indices),7),dtype=int)\n",
    "aft_neutral_x = np.zeros((len(neutral_indices),mfcc_size))\n",
    "aft_neutral_y = np.zeros((len(neutral_indices),7),dtype=int)\n",
    "for i in range(len(neutral_indices)):\n",
    "    bef_neutral_x[i] = np.resize(bef_x[neutral_indices[i]],(1,mfcc_size))\n",
    "    bef_neutral_y[i] = np.resize(bef_y[neutral_indices[i]],(1,7))\n",
    "    aft_neutral_x[i] = np.resize(aft_x[neutral_indices[i]],(1,mfcc_size))\n",
    "    aft_neutral_y[i] = np.resize(aft_y[neutral_indices[i]],(1,7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets divide the dataset into test/train/split as 60/20/20 and create a 3 layered dnn model with 128-256-128 nodes.\n",
    "model_a is for the dataset after the removal of silence.\n",
    "model_b is for the dataset before the removal of silemce(original audio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 36.4235 - accuracy: 0.1944 - val_loss: 9.1989 - val_accuracy: 0.3084\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 26.3968 - accuracy: 0.1411 - val_loss: 4.4018 - val_accuracy: 0.2710\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.6849 - accuracy: 0.1912 - val_loss: 2.0971 - val_accuracy: 0.2710\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 12.1950 - accuracy: 0.1787 - val_loss: 1.8211 - val_accuracy: 0.2617\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.7498 - accuracy: 0.2288 - val_loss: 1.7804 - val_accuracy: 0.2804\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.3562 - accuracy: 0.1912 - val_loss: 1.8726 - val_accuracy: 0.2430\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.9635 - accuracy: 0.2382 - val_loss: 1.8665 - val_accuracy: 0.2056\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.2523 - accuracy: 0.1944 - val_loss: 1.8860 - val_accuracy: 0.1495\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.0193 - accuracy: 0.1661 - val_loss: 1.8848 - val_accuracy: 0.1682\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1572 - accuracy: 0.1818 - val_loss: 1.8731 - val_accuracy: 0.1869\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.6665 - accuracy: 0.2132 - val_loss: 1.8553 - val_accuracy: 0.1776\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0405 - accuracy: 0.2320 - val_loss: 1.8486 - val_accuracy: 0.2336\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0184 - accuracy: 0.1912 - val_loss: 1.8522 - val_accuracy: 0.2991\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5613 - accuracy: 0.2414 - val_loss: 1.8440 - val_accuracy: 0.2991\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5048 - accuracy: 0.2539 - val_loss: 1.8467 - val_accuracy: 0.2897\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4811 - accuracy: 0.2382 - val_loss: 1.8433 - val_accuracy: 0.2897\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3839 - accuracy: 0.2257 - val_loss: 1.8426 - val_accuracy: 0.3084\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2583 - accuracy: 0.2665 - val_loss: 1.8470 - val_accuracy: 0.3084\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0145 - accuracy: 0.2853 - val_loss: 1.8419 - val_accuracy: 0.3271\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1254 - accuracy: 0.2759 - val_loss: 1.8427 - val_accuracy: 0.3178\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1249 - accuracy: 0.2884 - val_loss: 1.8428 - val_accuracy: 0.3364\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2234 - accuracy: 0.2194 - val_loss: 1.8387 - val_accuracy: 0.3364\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0790 - accuracy: 0.2382 - val_loss: 1.8288 - val_accuracy: 0.3458\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9380 - accuracy: 0.2759 - val_loss: 1.8246 - val_accuracy: 0.3645\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0628 - accuracy: 0.2571 - val_loss: 1.8237 - val_accuracy: 0.3271\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8571 - accuracy: 0.3041 - val_loss: 1.8221 - val_accuracy: 0.2991\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9372 - accuracy: 0.3448 - val_loss: 1.8087 - val_accuracy: 0.3084\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9087 - accuracy: 0.2915 - val_loss: 1.8009 - val_accuracy: 0.2897\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8901 - accuracy: 0.2382 - val_loss: 1.7991 - val_accuracy: 0.3271\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8309 - accuracy: 0.2727 - val_loss: 1.7914 - val_accuracy: 0.3458\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9008 - accuracy: 0.2320 - val_loss: 1.7822 - val_accuracy: 0.3832\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8288 - accuracy: 0.3292 - val_loss: 1.7757 - val_accuracy: 0.3738\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8111 - accuracy: 0.3041 - val_loss: 1.7610 - val_accuracy: 0.3832\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7779 - accuracy: 0.2915 - val_loss: 1.7457 - val_accuracy: 0.3738\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7795 - accuracy: 0.3511 - val_loss: 1.7359 - val_accuracy: 0.3925\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7641 - accuracy: 0.3166 - val_loss: 1.7155 - val_accuracy: 0.3832\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7430 - accuracy: 0.3292 - val_loss: 1.6926 - val_accuracy: 0.3645\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.6834 - accuracy: 0.3574 - val_loss: 1.6654 - val_accuracy: 0.3925\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7489 - accuracy: 0.3135 - val_loss: 1.6569 - val_accuracy: 0.3832\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.7851 - accuracy: 0.3229 - val_loss: 1.6644 - val_accuracy: 0.3925\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7703 - accuracy: 0.3323 - val_loss: 1.6667 - val_accuracy: 0.4112\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6581 - accuracy: 0.3292 - val_loss: 1.6496 - val_accuracy: 0.3925\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.6186 - accuracy: 0.4013 - val_loss: 1.6331 - val_accuracy: 0.4019\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7010 - accuracy: 0.3354 - val_loss: 1.6226 - val_accuracy: 0.4112\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6787 - accuracy: 0.3448 - val_loss: 1.6143 - val_accuracy: 0.4299\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6302 - accuracy: 0.3480 - val_loss: 1.6133 - val_accuracy: 0.4112\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5573 - accuracy: 0.4044 - val_loss: 1.5953 - val_accuracy: 0.4112\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6939 - accuracy: 0.3229 - val_loss: 1.5901 - val_accuracy: 0.3925\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6882 - accuracy: 0.3260 - val_loss: 1.5831 - val_accuracy: 0.3832\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.6883 - accuracy: 0.3511 - val_loss: 1.5692 - val_accuracy: 0.4299\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6681 - accuracy: 0.3699 - val_loss: 1.5486 - val_accuracy: 0.4299\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5472 - accuracy: 0.3856 - val_loss: 1.5488 - val_accuracy: 0.4579\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5439 - accuracy: 0.3856 - val_loss: 1.5346 - val_accuracy: 0.4393\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.5921 - accuracy: 0.3699 - val_loss: 1.5096 - val_accuracy: 0.4299\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5707 - accuracy: 0.3856 - val_loss: 1.4957 - val_accuracy: 0.4393\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5327 - accuracy: 0.3668 - val_loss: 1.4862 - val_accuracy: 0.4486\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5943 - accuracy: 0.3856 - val_loss: 1.4782 - val_accuracy: 0.4673\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5128 - accuracy: 0.4232 - val_loss: 1.4528 - val_accuracy: 0.4486\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5527 - accuracy: 0.3856 - val_loss: 1.4473 - val_accuracy: 0.4579\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5459 - accuracy: 0.3762 - val_loss: 1.4488 - val_accuracy: 0.4579\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5434 - accuracy: 0.4169 - val_loss: 1.4495 - val_accuracy: 0.4766\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.4075 - val_loss: 1.4420 - val_accuracy: 0.4673\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4729 - accuracy: 0.4295 - val_loss: 1.4377 - val_accuracy: 0.4579\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5502 - accuracy: 0.3511 - val_loss: 1.4370 - val_accuracy: 0.4860\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.5281 - accuracy: 0.3574 - val_loss: 1.4250 - val_accuracy: 0.4579\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4544 - accuracy: 0.4138 - val_loss: 1.4119 - val_accuracy: 0.4393\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4777 - accuracy: 0.3574 - val_loss: 1.3884 - val_accuracy: 0.4579\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4194 - accuracy: 0.4232 - val_loss: 1.3987 - val_accuracy: 0.4299\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4520 - accuracy: 0.4013 - val_loss: 1.3935 - val_accuracy: 0.4673\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4028 - accuracy: 0.4514 - val_loss: 1.3852 - val_accuracy: 0.4579\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4381 - accuracy: 0.4389 - val_loss: 1.3699 - val_accuracy: 0.4486\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3585 - accuracy: 0.4545 - val_loss: 1.3422 - val_accuracy: 0.4579\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4174 - accuracy: 0.4232 - val_loss: 1.3574 - val_accuracy: 0.4486\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3568 - accuracy: 0.4295 - val_loss: 1.3648 - val_accuracy: 0.4673\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4326 - accuracy: 0.4295 - val_loss: 1.3486 - val_accuracy: 0.4766\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3736 - accuracy: 0.4420 - val_loss: 1.3419 - val_accuracy: 0.4579\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3768 - accuracy: 0.4671 - val_loss: 1.3276 - val_accuracy: 0.4393\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3833 - accuracy: 0.4639 - val_loss: 1.3042 - val_accuracy: 0.4486\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4444 - accuracy: 0.4483 - val_loss: 1.3202 - val_accuracy: 0.4766\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3754 - accuracy: 0.4514 - val_loss: 1.3274 - val_accuracy: 0.4579\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3880 - accuracy: 0.4514 - val_loss: 1.3186 - val_accuracy: 0.4393\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.4487 - accuracy: 0.4420 - val_loss: 1.3227 - val_accuracy: 0.4486\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3651 - accuracy: 0.4295 - val_loss: 1.2812 - val_accuracy: 0.4393\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3898 - accuracy: 0.4451 - val_loss: 1.2601 - val_accuracy: 0.4953\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3185 - accuracy: 0.4639 - val_loss: 1.2830 - val_accuracy: 0.5047\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3024 - accuracy: 0.4890 - val_loss: 1.2692 - val_accuracy: 0.4860\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3233 - accuracy: 0.4608 - val_loss: 1.2429 - val_accuracy: 0.5047\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3104 - accuracy: 0.4577 - val_loss: 1.2349 - val_accuracy: 0.5234\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3483 - accuracy: 0.4765 - val_loss: 1.2764 - val_accuracy: 0.4860\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3189 - accuracy: 0.4577 - val_loss: 1.2970 - val_accuracy: 0.4766\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.3955 - accuracy: 0.4357 - val_loss: 1.2586 - val_accuracy: 0.4953\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2828 - accuracy: 0.4702 - val_loss: 1.2251 - val_accuracy: 0.5047\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2820 - accuracy: 0.4890 - val_loss: 1.2438 - val_accuracy: 0.4673\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.3535 - accuracy: 0.4483 - val_loss: 1.2412 - val_accuracy: 0.4860\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.2758 - accuracy: 0.4828 - val_loss: 1.2285 - val_accuracy: 0.4860\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2770 - accuracy: 0.4796 - val_loss: 1.2293 - val_accuracy: 0.4673\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2429 - accuracy: 0.5172 - val_loss: 1.2571 - val_accuracy: 0.5421\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2550 - accuracy: 0.5141 - val_loss: 1.2568 - val_accuracy: 0.5234\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.2842 - accuracy: 0.4953 - val_loss: 1.2515 - val_accuracy: 0.5327\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.2651 - accuracy: 0.5078 - val_loss: 1.2160 - val_accuracy: 0.5140\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2492 - accuracy: 0.5016 - val_loss: 1.2124 - val_accuracy: 0.5327\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2082 - accuracy: 0.5141 - val_loss: 1.1939 - val_accuracy: 0.5421\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2000 - accuracy: 0.4639 - val_loss: 1.2035 - val_accuracy: 0.5234\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.2183 - accuracy: 0.5172 - val_loss: 1.1983 - val_accuracy: 0.5607\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1889 - accuracy: 0.4922 - val_loss: 1.1935 - val_accuracy: 0.5421\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.2776 - accuracy: 0.4671 - val_loss: 1.2099 - val_accuracy: 0.5327\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1531 - accuracy: 0.5423 - val_loss: 1.2023 - val_accuracy: 0.5140\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2189 - accuracy: 0.5266 - val_loss: 1.2135 - val_accuracy: 0.5047\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1923 - accuracy: 0.5078 - val_loss: 1.2210 - val_accuracy: 0.4766\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1382 - accuracy: 0.5455 - val_loss: 1.2110 - val_accuracy: 0.5234\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1397 - accuracy: 0.5329 - val_loss: 1.1890 - val_accuracy: 0.5421\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1988 - accuracy: 0.5329 - val_loss: 1.1806 - val_accuracy: 0.5421\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.1972 - accuracy: 0.5016 - val_loss: 1.1909 - val_accuracy: 0.5140\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.1684 - accuracy: 0.5141 - val_loss: 1.1859 - val_accuracy: 0.5140\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1390 - accuracy: 0.5455 - val_loss: 1.1805 - val_accuracy: 0.5701\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2148 - accuracy: 0.5016 - val_loss: 1.1859 - val_accuracy: 0.5421\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1341 - accuracy: 0.5705 - val_loss: 1.2018 - val_accuracy: 0.5701\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.2063 - accuracy: 0.5423 - val_loss: 1.2216 - val_accuracy: 0.5234\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1197 - accuracy: 0.5361 - val_loss: 1.2001 - val_accuracy: 0.5327\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2019 - accuracy: 0.5392 - val_loss: 1.1924 - val_accuracy: 0.5981\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2008 - accuracy: 0.5141 - val_loss: 1.1740 - val_accuracy: 0.5701\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1926 - accuracy: 0.5392 - val_loss: 1.1790 - val_accuracy: 0.5794\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2552 - accuracy: 0.5047 - val_loss: 1.1880 - val_accuracy: 0.5607\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1858 - accuracy: 0.5455 - val_loss: 1.1922 - val_accuracy: 0.5421\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1859 - accuracy: 0.4922 - val_loss: 1.1896 - val_accuracy: 0.5514\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.1851 - accuracy: 0.5517 - val_loss: 1.1827 - val_accuracy: 0.5514\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1082 - accuracy: 0.5987 - val_loss: 1.1629 - val_accuracy: 0.5514\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2135 - accuracy: 0.5266 - val_loss: 1.1686 - val_accuracy: 0.5421\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0869 - accuracy: 0.5329 - val_loss: 1.1678 - val_accuracy: 0.5607\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.0991 - accuracy: 0.5392 - val_loss: 1.1515 - val_accuracy: 0.5327\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1253 - accuracy: 0.5611 - val_loss: 1.1677 - val_accuracy: 0.5888\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0651 - accuracy: 0.5392 - val_loss: 1.1387 - val_accuracy: 0.5607\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0599 - accuracy: 0.5956 - val_loss: 1.1195 - val_accuracy: 0.5514\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0796 - accuracy: 0.5611 - val_loss: 1.1077 - val_accuracy: 0.5327\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0353 - accuracy: 0.5611 - val_loss: 1.1363 - val_accuracy: 0.5234\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.9421 - accuracy: 0.6364 - val_loss: 1.1378 - val_accuracy: 0.5421\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1258 - accuracy: 0.5580 - val_loss: 1.1191 - val_accuracy: 0.5607\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0492 - accuracy: 0.6176 - val_loss: 1.1231 - val_accuracy: 0.5794\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1549 - accuracy: 0.5361 - val_loss: 1.1076 - val_accuracy: 0.5888\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0689 - accuracy: 0.5643 - val_loss: 1.1192 - val_accuracy: 0.5794\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0818 - accuracy: 0.5768 - val_loss: 1.1311 - val_accuracy: 0.5514\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0694 - accuracy: 0.5831 - val_loss: 1.1205 - val_accuracy: 0.5421\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.0217 - accuracy: 0.6176 - val_loss: 1.1017 - val_accuracy: 0.5701\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9933 - accuracy: 0.5611 - val_loss: 1.1117 - val_accuracy: 0.5701\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9899 - accuracy: 0.6238 - val_loss: 1.0970 - val_accuracy: 0.5701\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0438 - accuracy: 0.5674 - val_loss: 1.1013 - val_accuracy: 0.5607\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9374 - accuracy: 0.5987 - val_loss: 1.1301 - val_accuracy: 0.5421\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9712 - accuracy: 0.5893 - val_loss: 1.1164 - val_accuracy: 0.6075\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0408 - accuracy: 0.6270 - val_loss: 1.1501 - val_accuracy: 0.5514\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9957 - accuracy: 0.5893 - val_loss: 1.1860 - val_accuracy: 0.5701\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0103 - accuracy: 0.6019 - val_loss: 1.1767 - val_accuracy: 0.5981\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8781 - accuracy: 0.5987 - val_loss: 1.1569 - val_accuracy: 0.5701\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9974 - accuracy: 0.5674 - val_loss: 1.1788 - val_accuracy: 0.5794\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9712 - accuracy: 0.6082 - val_loss: 1.1061 - val_accuracy: 0.5888\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9909 - accuracy: 0.5893 - val_loss: 1.1107 - val_accuracy: 0.5794\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0045 - accuracy: 0.6144 - val_loss: 1.1052 - val_accuracy: 0.6168\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9905 - accuracy: 0.6019 - val_loss: 1.1131 - val_accuracy: 0.5794\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9595 - accuracy: 0.5925 - val_loss: 1.0591 - val_accuracy: 0.6542\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9336 - accuracy: 0.6301 - val_loss: 1.0711 - val_accuracy: 0.5888\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9476 - accuracy: 0.6207 - val_loss: 1.0969 - val_accuracy: 0.5794\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9828 - accuracy: 0.5956 - val_loss: 1.0922 - val_accuracy: 0.6075\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9644 - accuracy: 0.5737 - val_loss: 1.0922 - val_accuracy: 0.5981\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9326 - accuracy: 0.6050 - val_loss: 1.1127 - val_accuracy: 0.6168\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8913 - accuracy: 0.6552 - val_loss: 1.1298 - val_accuracy: 0.5607\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9556 - accuracy: 0.6458 - val_loss: 1.1409 - val_accuracy: 0.6075\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9481 - accuracy: 0.6238 - val_loss: 1.1499 - val_accuracy: 0.6636\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8759 - accuracy: 0.6552 - val_loss: 1.1464 - val_accuracy: 0.6075\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9547 - accuracy: 0.5987 - val_loss: 1.1927 - val_accuracy: 0.6075\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9495 - accuracy: 0.6301 - val_loss: 1.1906 - val_accuracy: 0.5794\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9126 - accuracy: 0.6426 - val_loss: 1.1467 - val_accuracy: 0.5421\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8890 - accuracy: 0.6364 - val_loss: 1.2005 - val_accuracy: 0.5794\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9453 - accuracy: 0.6238 - val_loss: 1.1890 - val_accuracy: 0.5607\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0010 - accuracy: 0.6019 - val_loss: 1.0626 - val_accuracy: 0.6075\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8885 - accuracy: 0.6834 - val_loss: 1.0397 - val_accuracy: 0.6262\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9130 - accuracy: 0.6489 - val_loss: 1.0899 - val_accuracy: 0.5514\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8831 - accuracy: 0.6583 - val_loss: 1.0709 - val_accuracy: 0.5888\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9120 - accuracy: 0.6520 - val_loss: 1.1209 - val_accuracy: 0.5421\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8778 - accuracy: 0.6301 - val_loss: 1.1618 - val_accuracy: 0.5888\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8612 - accuracy: 0.6301 - val_loss: 1.1467 - val_accuracy: 0.5701\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - accuracy: 0.6207 - val_loss: 1.1751 - val_accuracy: 0.6075\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8771 - accuracy: 0.6489 - val_loss: 1.1869 - val_accuracy: 0.5701\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8511 - accuracy: 0.6552 - val_loss: 1.0691 - val_accuracy: 0.5514\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9477 - accuracy: 0.6050 - val_loss: 1.0741 - val_accuracy: 0.5701\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - accuracy: 0.6395 - val_loss: 1.0986 - val_accuracy: 0.5981\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8229 - accuracy: 0.6332 - val_loss: 1.1293 - val_accuracy: 0.5514\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8139 - accuracy: 0.6458 - val_loss: 1.1619 - val_accuracy: 0.6075\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8659 - accuracy: 0.6332 - val_loss: 1.1397 - val_accuracy: 0.5888\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8446 - accuracy: 0.6677 - val_loss: 1.1047 - val_accuracy: 0.5514\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8129 - accuracy: 0.6238 - val_loss: 1.1882 - val_accuracy: 0.6075\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8497 - accuracy: 0.6364 - val_loss: 1.0924 - val_accuracy: 0.5888\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - accuracy: 0.6176 - val_loss: 1.0698 - val_accuracy: 0.5794\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - accuracy: 0.6364 - val_loss: 1.1628 - val_accuracy: 0.5981\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8062 - accuracy: 0.6865 - val_loss: 1.1368 - val_accuracy: 0.6075\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8631 - accuracy: 0.6364 - val_loss: 1.1149 - val_accuracy: 0.6355\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8031 - accuracy: 0.6740 - val_loss: 1.1081 - val_accuracy: 0.6168\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7695 - accuracy: 0.6897 - val_loss: 1.0795 - val_accuracy: 0.6542\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - accuracy: 0.6458 - val_loss: 1.1077 - val_accuracy: 0.6449\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8154 - accuracy: 0.6646 - val_loss: 1.1167 - val_accuracy: 0.5981\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - accuracy: 0.6552 - val_loss: 1.1371 - val_accuracy: 0.5514\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7896 - accuracy: 0.6740 - val_loss: 1.1201 - val_accuracy: 0.5981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1946ca864a8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(aft_x, aft_y, test_size=0.2, random_state=42)\n",
    "Xa_train, Xa_valid, ya_train, ya_valid = train_test_split(Xa_train, ya_train, test_size=0.25, random_state=42)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "num_labels = 7\n",
    "filter_size = 2\n",
    "\n",
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Dense(128, input_shape=(mfcc_size,)))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(256))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(128))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(num_labels))\n",
    "model_a.add(Activation('softmax'))\n",
    "\n",
    "model_a.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "\n",
    "model_a.fit(Xa_train, ya_train, batch_size=30, epochs=200, validation_data=(Xa_valid,ya_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of the model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6355140186915887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predict_ya= model_a.predict(Xa_test )\n",
    "np.where(np.argmax(predict_ya[:]),1,0)\n",
    "for i in range(len(predict_ya)):\n",
    "    temp = np.argmax(predict_ya[i])\n",
    "    predict_ya[i] = np.zeros((1,7))\n",
    "    predict_ya[i][temp] = 1\n",
    "aft_acc=accuracy_score(ya_test,predict_ya)\n",
    "print(aft_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with model_b with Dataset before silence removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 40.0898 - accuracy: 0.1724 - val_loss: 9.6399 - val_accuracy: 0.2336\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 29.5270 - accuracy: 0.1285 - val_loss: 5.2471 - val_accuracy: 0.2991\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.8766 - accuracy: 0.1724 - val_loss: 2.3935 - val_accuracy: 0.2804\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.1068 - accuracy: 0.1755 - val_loss: 1.9525 - val_accuracy: 0.2243\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.0341 - accuracy: 0.1850 - val_loss: 1.8064 - val_accuracy: 0.2897\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 8.8770 - accuracy: 0.1129 - val_loss: 1.8893 - val_accuracy: 0.1869\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.5985 - accuracy: 0.1787 - val_loss: 1.8561 - val_accuracy: 0.1963\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.3248 - accuracy: 0.1724 - val_loss: 1.8778 - val_accuracy: 0.1963\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.5356 - accuracy: 0.2132 - val_loss: 1.8924 - val_accuracy: 0.1869\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.5337 - accuracy: 0.1944 - val_loss: 1.8883 - val_accuracy: 0.1682\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0824 - accuracy: 0.1912 - val_loss: 1.8802 - val_accuracy: 0.1402\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.5678 - accuracy: 0.2351 - val_loss: 1.8804 - val_accuracy: 0.2150\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.3416 - accuracy: 0.2038 - val_loss: 1.8929 - val_accuracy: 0.2336\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.8194 - accuracy: 0.2288 - val_loss: 1.8977 - val_accuracy: 0.2336\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8030 - accuracy: 0.1818 - val_loss: 1.8996 - val_accuracy: 0.2243\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6374 - accuracy: 0.2633 - val_loss: 1.8853 - val_accuracy: 0.2243\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4891 - accuracy: 0.2571 - val_loss: 1.8757 - val_accuracy: 0.2336\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4761 - accuracy: 0.2320 - val_loss: 1.8710 - val_accuracy: 0.2336\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0941 - accuracy: 0.2884 - val_loss: 1.8636 - val_accuracy: 0.2336\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2845 - accuracy: 0.2351 - val_loss: 1.8547 - val_accuracy: 0.3458\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2066 - accuracy: 0.2665 - val_loss: 1.8508 - val_accuracy: 0.3458\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1551 - accuracy: 0.2163 - val_loss: 1.8451 - val_accuracy: 0.3458\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0230 - accuracy: 0.2665 - val_loss: 1.8274 - val_accuracy: 0.3458\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0400 - accuracy: 0.2414 - val_loss: 1.8120 - val_accuracy: 0.3364\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1357 - accuracy: 0.2414 - val_loss: 1.8109 - val_accuracy: 0.3364\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9271 - accuracy: 0.2947 - val_loss: 1.8040 - val_accuracy: 0.3364\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0483 - accuracy: 0.2759 - val_loss: 1.7932 - val_accuracy: 0.3364\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9596 - accuracy: 0.2665 - val_loss: 1.7916 - val_accuracy: 0.3364\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0103 - accuracy: 0.2288 - val_loss: 1.7887 - val_accuracy: 0.3458\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9286 - accuracy: 0.2915 - val_loss: 1.7862 - val_accuracy: 0.3551\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9495 - accuracy: 0.2226 - val_loss: 1.7709 - val_accuracy: 0.3551\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8036 - accuracy: 0.3009 - val_loss: 1.7620 - val_accuracy: 0.3551\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8961 - accuracy: 0.2821 - val_loss: 1.7452 - val_accuracy: 0.3551\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8086 - accuracy: 0.2665 - val_loss: 1.7259 - val_accuracy: 0.3551\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8959 - accuracy: 0.2821 - val_loss: 1.7311 - val_accuracy: 0.3551\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8012 - accuracy: 0.3009 - val_loss: 1.7300 - val_accuracy: 0.3551\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7916 - accuracy: 0.3354 - val_loss: 1.7230 - val_accuracy: 0.3551\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7766 - accuracy: 0.2915 - val_loss: 1.7032 - val_accuracy: 0.3551\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7926 - accuracy: 0.3166 - val_loss: 1.7065 - val_accuracy: 0.3551\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8660 - accuracy: 0.3009 - val_loss: 1.7142 - val_accuracy: 0.3551\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8191 - accuracy: 0.2727 - val_loss: 1.7012 - val_accuracy: 0.3551\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7153 - accuracy: 0.3166 - val_loss: 1.6617 - val_accuracy: 0.3551\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7772 - accuracy: 0.3386 - val_loss: 1.6568 - val_accuracy: 0.3551\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7995 - accuracy: 0.3103 - val_loss: 1.6534 - val_accuracy: 0.3551\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7898 - accuracy: 0.3072 - val_loss: 1.6397 - val_accuracy: 0.3551\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7062 - accuracy: 0.3417 - val_loss: 1.6172 - val_accuracy: 0.3551\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6941 - accuracy: 0.3762 - val_loss: 1.6232 - val_accuracy: 0.3551\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6981 - accuracy: 0.3323 - val_loss: 1.6420 - val_accuracy: 0.3551\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7443 - accuracy: 0.2884 - val_loss: 1.6141 - val_accuracy: 0.3551\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6658 - accuracy: 0.3292 - val_loss: 1.6073 - val_accuracy: 0.3551\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7368 - accuracy: 0.2978 - val_loss: 1.6032 - val_accuracy: 0.3645\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5704 - accuracy: 0.3636 - val_loss: 1.5949 - val_accuracy: 0.3645\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6626 - accuracy: 0.3386 - val_loss: 1.5945 - val_accuracy: 0.3551\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7116 - accuracy: 0.3511 - val_loss: 1.5897 - val_accuracy: 0.3458\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.6082 - accuracy: 0.3730 - val_loss: 1.5723 - val_accuracy: 0.3645\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6091 - accuracy: 0.3354 - val_loss: 1.5344 - val_accuracy: 0.3645\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.5429 - accuracy: 0.4169 - val_loss: 1.5300 - val_accuracy: 0.3645\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5453 - accuracy: 0.3793 - val_loss: 1.5043 - val_accuracy: 0.3832\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6247 - accuracy: 0.3417 - val_loss: 1.4915 - val_accuracy: 0.3832\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5293 - accuracy: 0.3605 - val_loss: 1.4947 - val_accuracy: 0.4019\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4834 - accuracy: 0.3636 - val_loss: 1.4655 - val_accuracy: 0.4019\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5353 - accuracy: 0.4107 - val_loss: 1.4601 - val_accuracy: 0.4299\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5100 - accuracy: 0.3856 - val_loss: 1.4376 - val_accuracy: 0.4299\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5770 - accuracy: 0.3668 - val_loss: 1.4310 - val_accuracy: 0.4766\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4815 - accuracy: 0.3699 - val_loss: 1.4174 - val_accuracy: 0.4953\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4886 - accuracy: 0.3918 - val_loss: 1.4018 - val_accuracy: 0.4766\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4863 - accuracy: 0.4044 - val_loss: 1.3990 - val_accuracy: 0.4579\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4925 - accuracy: 0.3856 - val_loss: 1.3899 - val_accuracy: 0.4486\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5510 - accuracy: 0.3887 - val_loss: 1.3960 - val_accuracy: 0.4860\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3932 - accuracy: 0.4295 - val_loss: 1.3808 - val_accuracy: 0.4486\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4774 - accuracy: 0.3699 - val_loss: 1.3604 - val_accuracy: 0.4766\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3765 - accuracy: 0.4514 - val_loss: 1.3598 - val_accuracy: 0.5140\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4572 - accuracy: 0.3699 - val_loss: 1.3317 - val_accuracy: 0.4860\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4056 - accuracy: 0.4232 - val_loss: 1.3358 - val_accuracy: 0.5047\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4125 - accuracy: 0.4420 - val_loss: 1.3440 - val_accuracy: 0.4953\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4602 - accuracy: 0.3856 - val_loss: 1.3409 - val_accuracy: 0.5047\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3706 - accuracy: 0.4389 - val_loss: 1.3324 - val_accuracy: 0.5140\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4199 - accuracy: 0.4451 - val_loss: 1.3275 - val_accuracy: 0.4860\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3841 - accuracy: 0.4201 - val_loss: 1.3301 - val_accuracy: 0.5047\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4011 - accuracy: 0.4075 - val_loss: 1.3207 - val_accuracy: 0.4953\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3961 - accuracy: 0.4138 - val_loss: 1.2953 - val_accuracy: 0.4953\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4552 - accuracy: 0.4295 - val_loss: 1.2825 - val_accuracy: 0.5140\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3934 - accuracy: 0.4232 - val_loss: 1.2684 - val_accuracy: 0.5234\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4002 - accuracy: 0.4545 - val_loss: 1.2783 - val_accuracy: 0.5047\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3506 - accuracy: 0.4201 - val_loss: 1.2907 - val_accuracy: 0.5327\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3980 - accuracy: 0.4295 - val_loss: 1.2822 - val_accuracy: 0.5234\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3501 - accuracy: 0.4295 - val_loss: 1.2751 - val_accuracy: 0.5234\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3753 - accuracy: 0.4295 - val_loss: 1.2639 - val_accuracy: 0.5327\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3440 - accuracy: 0.4483 - val_loss: 1.2788 - val_accuracy: 0.4953\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3048 - accuracy: 0.4483 - val_loss: 1.2614 - val_accuracy: 0.5234\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2758 - accuracy: 0.4169 - val_loss: 1.2589 - val_accuracy: 0.5607\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2978 - accuracy: 0.4734 - val_loss: 1.2306 - val_accuracy: 0.5140\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3789 - accuracy: 0.4232 - val_loss: 1.2363 - val_accuracy: 0.5327\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3560 - accuracy: 0.4201 - val_loss: 1.2304 - val_accuracy: 0.5327\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2982 - accuracy: 0.4734 - val_loss: 1.2289 - val_accuracy: 0.5140\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2705 - accuracy: 0.4953 - val_loss: 1.2174 - val_accuracy: 0.5140\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2577 - accuracy: 0.4514 - val_loss: 1.2159 - val_accuracy: 0.5327\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2933 - accuracy: 0.4577 - val_loss: 1.1998 - val_accuracy: 0.5421\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2745 - accuracy: 0.4671 - val_loss: 1.2151 - val_accuracy: 0.5234\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3394 - accuracy: 0.4483 - val_loss: 1.2012 - val_accuracy: 0.4953\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2644 - accuracy: 0.4828 - val_loss: 1.1852 - val_accuracy: 0.5047\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2645 - accuracy: 0.4734 - val_loss: 1.1800 - val_accuracy: 0.5327\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2385 - accuracy: 0.5016 - val_loss: 1.1699 - val_accuracy: 0.5421\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2086 - accuracy: 0.5110 - val_loss: 1.1591 - val_accuracy: 0.5607\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2533 - accuracy: 0.4639 - val_loss: 1.1652 - val_accuracy: 0.5421\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2204 - accuracy: 0.4922 - val_loss: 1.1544 - val_accuracy: 0.5514\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2156 - accuracy: 0.5110 - val_loss: 1.1301 - val_accuracy: 0.5514\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3093 - accuracy: 0.4514 - val_loss: 1.1318 - val_accuracy: 0.5888\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2362 - accuracy: 0.4514 - val_loss: 1.1734 - val_accuracy: 0.5140\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2118 - accuracy: 0.4953 - val_loss: 1.1542 - val_accuracy: 0.5514\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1604 - accuracy: 0.4890 - val_loss: 1.1572 - val_accuracy: 0.5888\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0950 - accuracy: 0.5235 - val_loss: 1.1412 - val_accuracy: 0.5794\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1739 - accuracy: 0.5047 - val_loss: 1.1313 - val_accuracy: 0.5888\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.1588 - accuracy: 0.5486 - val_loss: 1.1299 - val_accuracy: 0.5701\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1717 - accuracy: 0.5110 - val_loss: 1.1145 - val_accuracy: 0.5701\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1727 - accuracy: 0.5329 - val_loss: 1.1198 - val_accuracy: 0.5514\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1690 - accuracy: 0.5298 - val_loss: 1.1210 - val_accuracy: 0.6075\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2048 - accuracy: 0.4545 - val_loss: 1.1125 - val_accuracy: 0.5794\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.1702 - accuracy: 0.5235 - val_loss: 1.0936 - val_accuracy: 0.6168\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.1194 - accuracy: 0.5361 - val_loss: 1.0859 - val_accuracy: 0.5794\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1645 - accuracy: 0.5141 - val_loss: 1.0810 - val_accuracy: 0.5701\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1500 - accuracy: 0.5235 - val_loss: 1.0877 - val_accuracy: 0.5981\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1014 - accuracy: 0.5455 - val_loss: 1.0808 - val_accuracy: 0.6168\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1103 - accuracy: 0.5329 - val_loss: 1.0598 - val_accuracy: 0.6168\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0717 - accuracy: 0.5517 - val_loss: 1.0619 - val_accuracy: 0.5794\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1423 - accuracy: 0.5361 - val_loss: 1.0588 - val_accuracy: 0.6075\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1249 - accuracy: 0.5423 - val_loss: 1.0598 - val_accuracy: 0.5888\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0516 - accuracy: 0.5674 - val_loss: 1.0600 - val_accuracy: 0.5888\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0443 - accuracy: 0.6050 - val_loss: 1.0692 - val_accuracy: 0.5607\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9749 - accuracy: 0.6364 - val_loss: 1.0638 - val_accuracy: 0.5514\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0676 - accuracy: 0.5799 - val_loss: 1.0220 - val_accuracy: 0.6075\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9929 - accuracy: 0.5831 - val_loss: 1.0476 - val_accuracy: 0.6168\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.5674 - val_loss: 1.0268 - val_accuracy: 0.5981\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1267 - accuracy: 0.5329 - val_loss: 1.0554 - val_accuracy: 0.5701\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0061 - accuracy: 0.6144 - val_loss: 1.0553 - val_accuracy: 0.5794\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0508 - accuracy: 0.5737 - val_loss: 1.0175 - val_accuracy: 0.6542\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0269 - accuracy: 0.5893 - val_loss: 1.0144 - val_accuracy: 0.6355\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0192 - accuracy: 0.5455 - val_loss: 1.0161 - val_accuracy: 0.6168\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0207 - accuracy: 0.5768 - val_loss: 1.0301 - val_accuracy: 0.5701\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0450 - accuracy: 0.5674 - val_loss: 1.0476 - val_accuracy: 0.5888\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0555 - accuracy: 0.5831 - val_loss: 1.0272 - val_accuracy: 0.6168\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0086 - accuracy: 0.5956 - val_loss: 1.0268 - val_accuracy: 0.6262\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0224 - accuracy: 0.6019 - val_loss: 0.9950 - val_accuracy: 0.6542\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9229 - accuracy: 0.5987 - val_loss: 1.0457 - val_accuracy: 0.5981\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9282 - accuracy: 0.5831 - val_loss: 1.0593 - val_accuracy: 0.6542\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9206 - accuracy: 0.6176 - val_loss: 0.9938 - val_accuracy: 0.6449\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9758 - accuracy: 0.6176 - val_loss: 0.9767 - val_accuracy: 0.5981\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0086 - accuracy: 0.6082 - val_loss: 1.0615 - val_accuracy: 0.6355\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9582 - accuracy: 0.5925 - val_loss: 1.0263 - val_accuracy: 0.6355\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0352 - accuracy: 0.5799 - val_loss: 0.9698 - val_accuracy: 0.6262\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9368 - accuracy: 0.6238 - val_loss: 0.9888 - val_accuracy: 0.6355\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9223 - accuracy: 0.6207 - val_loss: 0.9847 - val_accuracy: 0.6168\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9325 - accuracy: 0.6301 - val_loss: 1.0096 - val_accuracy: 0.5888\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9318 - accuracy: 0.6113 - val_loss: 0.9691 - val_accuracy: 0.6449\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9360 - accuracy: 0.5705 - val_loss: 0.9596 - val_accuracy: 0.7009\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9337 - accuracy: 0.6207 - val_loss: 1.0359 - val_accuracy: 0.6262\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9396 - accuracy: 0.6520 - val_loss: 0.9691 - val_accuracy: 0.6262\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8700 - accuracy: 0.6364 - val_loss: 1.0209 - val_accuracy: 0.6822\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8598 - accuracy: 0.6395 - val_loss: 0.9744 - val_accuracy: 0.6449\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8707 - accuracy: 0.6395 - val_loss: 0.9730 - val_accuracy: 0.6262\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9013 - accuracy: 0.6207 - val_loss: 0.9664 - val_accuracy: 0.6355\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8559 - accuracy: 0.6489 - val_loss: 0.9854 - val_accuracy: 0.6355\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8971 - accuracy: 0.6520 - val_loss: 0.9485 - val_accuracy: 0.6729\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8671 - accuracy: 0.6019 - val_loss: 0.9708 - val_accuracy: 0.6636\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8606 - accuracy: 0.6144 - val_loss: 0.9632 - val_accuracy: 0.6916\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - accuracy: 0.6677 - val_loss: 0.9247 - val_accuracy: 0.6262\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8686 - accuracy: 0.6332 - val_loss: 1.0473 - val_accuracy: 0.6542\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8287 - accuracy: 0.6583 - val_loss: 0.9978 - val_accuracy: 0.6916\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9243 - accuracy: 0.6458 - val_loss: 1.0004 - val_accuracy: 0.6168\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9308 - accuracy: 0.5799 - val_loss: 0.9931 - val_accuracy: 0.7009\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8922 - accuracy: 0.5893 - val_loss: 0.9314 - val_accuracy: 0.6729\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9261 - accuracy: 0.5893 - val_loss: 0.9410 - val_accuracy: 0.6075\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8164 - accuracy: 0.6552 - val_loss: 0.9440 - val_accuracy: 0.6262\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8236 - accuracy: 0.6552 - val_loss: 0.8727 - val_accuracy: 0.6542\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8842 - accuracy: 0.6332 - val_loss: 0.9685 - val_accuracy: 0.7103\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8119 - accuracy: 0.6426 - val_loss: 0.9819 - val_accuracy: 0.6916\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8080 - accuracy: 0.6395 - val_loss: 0.9372 - val_accuracy: 0.6262\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8146 - accuracy: 0.6552 - val_loss: 1.1177 - val_accuracy: 0.7477\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8798 - accuracy: 0.6395 - val_loss: 0.9425 - val_accuracy: 0.6355\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7908 - accuracy: 0.6771 - val_loss: 0.9359 - val_accuracy: 0.6542\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8017 - accuracy: 0.6865 - val_loss: 1.0312 - val_accuracy: 0.6355\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8073 - accuracy: 0.6646 - val_loss: 0.9510 - val_accuracy: 0.6636\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8532 - accuracy: 0.6646 - val_loss: 0.9608 - val_accuracy: 0.6916\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8804 - accuracy: 0.6834 - val_loss: 0.9604 - val_accuracy: 0.6075\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8769 - accuracy: 0.5956 - val_loss: 0.9359 - val_accuracy: 0.6355\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7812 - accuracy: 0.6740 - val_loss: 0.9998 - val_accuracy: 0.5981\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7941 - accuracy: 0.7053 - val_loss: 0.9535 - val_accuracy: 0.6449\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7330 - accuracy: 0.6614 - val_loss: 0.9535 - val_accuracy: 0.6729\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7437 - accuracy: 0.6803 - val_loss: 0.9374 - val_accuracy: 0.6916\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7747 - accuracy: 0.6928 - val_loss: 0.9450 - val_accuracy: 0.6916\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7743 - accuracy: 0.6489 - val_loss: 0.9053 - val_accuracy: 0.6168\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8566 - accuracy: 0.6489 - val_loss: 0.9003 - val_accuracy: 0.6262\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8018 - accuracy: 0.6740 - val_loss: 0.9406 - val_accuracy: 0.6729\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7360 - accuracy: 0.6740 - val_loss: 0.9855 - val_accuracy: 0.6729\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7638 - accuracy: 0.6771 - val_loss: 0.9217 - val_accuracy: 0.6822\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7727 - accuracy: 0.6834 - val_loss: 0.9049 - val_accuracy: 0.6729\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7184 - accuracy: 0.6489 - val_loss: 0.9273 - val_accuracy: 0.7196\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7520 - accuracy: 0.6928 - val_loss: 0.9082 - val_accuracy: 0.6916\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7353 - accuracy: 0.6708 - val_loss: 0.9870 - val_accuracy: 0.6916\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7933 - accuracy: 0.6646 - val_loss: 0.9573 - val_accuracy: 0.7009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1946de53320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(bef_x, bef_y, test_size=0.2, random_state=42)\n",
    "Xb_train, Xb_valid, yb_train, yb_valid = train_test_split(Xb_train, yb_train, test_size=0.25, random_state=42)\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "\n",
    "num_labels = 7\n",
    "filter_size = 2\n",
    "\n",
    "model_b = Sequential()\n",
    "\n",
    "model_b.add(Dense(128, input_shape=(mfcc_size,)))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(256))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(128))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(num_labels))\n",
    "model_b.add(Activation('softmax'))\n",
    "\n",
    "model_b.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model_b.fit(Xb_train, yb_train, batch_size=30, epochs=200, validation_data=(Xb_valid,yb_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5700934579439252\n"
     ]
    }
   ],
   "source": [
    "predict_yb= model_b.predict(Xb_test )\n",
    "np.where(np.argmax(predict_yb[:]),1,0)\n",
    "for i in range(len(predict_yb)):\n",
    "    temp = np.argmax(predict_yb[i])\n",
    "    predict_yb[i] = np.zeros((1,7))\n",
    "    predict_yb[i][temp] = 1\n",
    "bef_acc=accuracy_score(yb_test,predict_yb)\n",
    "print(bef_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here comes the main part . Calculating the accuracies of predicting different emotions before and after silence removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[0 for i in range(2)] for j in range(7)] \n",
    "predict_angry_b= model_b.predict(bef_angry_x)\n",
    "np.where(np.argmax(predict_angry_b[:]),1,0)\n",
    "for i in range(len(predict_angry_b)):\n",
    "    temp = np.argmax(predict_angry_b[i])\n",
    "    predict_angry_b[i] = np.zeros((1,7))\n",
    "    predict_angry_b[i][temp] = 1\n",
    "accuracies[0][0]=round((accuracy_score(bef_angry_y,predict_angry_b)),2)\n",
    "\n",
    "predict_angry_a= model_a.predict(aft_angry_x)\n",
    "np.where(np.argmax(predict_angry_a[:]),1,0)\n",
    "for i in range(len(predict_angry_a)):\n",
    "    temp = np.argmax(predict_angry_a[i])\n",
    "    predict_angry_a[i] = np.zeros((1,7))\n",
    "    predict_angry_a[i][temp] = 1\n",
    "accuracies[0][1]=round(accuracy_score(aft_angry_y,predict_angry_a),2)\n",
    "\n",
    "predict_boredom_b= model_b.predict(bef_boredom_x)\n",
    "np.where(np.argmax(predict_boredom_b[:]),1,0)\n",
    "for i in range(len(predict_boredom_b)):\n",
    "    temp = np.argmax(predict_boredom_b[i])\n",
    "    predict_boredom_b[i] = np.zeros((1,7))\n",
    "    predict_boredom_b[i][temp] = 1\n",
    "accuracies[1][0]=round(accuracy_score(bef_boredom_y,predict_boredom_b),2)\n",
    "\n",
    "predict_boredom_a= model_a.predict(aft_boredom_x)\n",
    "np.where(np.argmax(predict_boredom_a[:]),1,0)\n",
    "for i in range(len(predict_boredom_a)):\n",
    "    temp = np.argmax(predict_boredom_a[i])\n",
    "    predict_boredom_a[i] = np.zeros((1,7))\n",
    "    predict_boredom_a[i][temp] = 1\n",
    "accuracies[1][1]=round(accuracy_score(aft_boredom_y,predict_boredom_a),2)\n",
    "\n",
    "predict_disgust_b= model_b.predict(bef_disgust_x)\n",
    "np.where(np.argmax(predict_disgust_b[:]),1,0)\n",
    "for i in range(len(predict_disgust_b)):\n",
    "    temp = np.argmax(predict_disgust_b[i])\n",
    "    predict_disgust_b[i] = np.zeros((1,7))\n",
    "    predict_disgust_b[i][temp] = 1\n",
    "accuracies[2][0]=round(accuracy_score(bef_disgust_y,predict_disgust_b),2)\n",
    "\n",
    "predict_disgust_a= model_a.predict(aft_disgust_x)\n",
    "np.where(np.argmax(predict_disgust_a[:]),1,0)\n",
    "for i in range(len(predict_disgust_a)):\n",
    "    temp = np.argmax(predict_disgust_a[i])\n",
    "    predict_disgust_a[i] = np.zeros((1,7))\n",
    "    predict_disgust_a[i][temp] = 1\n",
    "accuracies[2][1]=round(accuracy_score(aft_disgust_y,predict_disgust_a),2)\n",
    "\n",
    "predict_fear_b= model_b.predict(bef_fear_x)\n",
    "np.where(np.argmax(predict_fear_b[:]),1,0)\n",
    "for i in range(len(predict_fear_b)):\n",
    "    temp = np.argmax(predict_fear_b[i])\n",
    "    predict_fear_b[i] = np.zeros((1,7))\n",
    "    predict_fear_b[i][temp] = 1\n",
    "accuracies[3][0]=round(accuracy_score(bef_fear_y,predict_fear_b),2)\n",
    "\n",
    "predict_fear_a= model_a.predict(aft_fear_x)\n",
    "np.where(np.argmax(predict_fear_a[:]),1,0)\n",
    "for i in range(len(predict_fear_a)):\n",
    "    temp = np.argmax(predict_fear_a[i])\n",
    "    predict_fear_a[i] = np.zeros((1,7))\n",
    "    predict_fear_a[i][temp] = 1\n",
    "accuracies[3][1]=round(accuracy_score(aft_fear_y,predict_fear_a),2)\n",
    "\n",
    "predict_happy_b= model_b.predict(bef_happy_x)\n",
    "np.where(np.argmax(predict_happy_b[:]),1,0)\n",
    "for i in range(len(predict_happy_b)):\n",
    "    temp = np.argmax(predict_happy_b[i])\n",
    "    predict_happy_b[i] = np.zeros((1,7))\n",
    "    predict_happy_b[i][temp] = 1\n",
    "accuracies[4][0]=round(accuracy_score(bef_happy_y,predict_happy_b),2)\n",
    "\n",
    "predict_happy_a= model_a.predict(aft_happy_x)\n",
    "np.where(np.argmax(predict_happy_a[:]),1,0)\n",
    "for i in range(len(predict_happy_a)):\n",
    "    temp = np.argmax(predict_happy_a[i])\n",
    "    predict_happy_a[i] = np.zeros((1,7))\n",
    "    predict_happy_a[i][temp] = 1\n",
    "accuracies[4][1]=round(accuracy_score(aft_happy_y,predict_happy_a),2)\n",
    "\n",
    "predict_sad_b= model_b.predict(bef_sad_x)\n",
    "np.where(np.argmax(predict_sad_b[:]),1,0)\n",
    "for i in range(len(predict_sad_b)):\n",
    "    temp = np.argmax(predict_sad_b[i])\n",
    "    predict_sad_b[i] = np.zeros((1,7))\n",
    "    predict_sad_b[i][temp] = 1\n",
    "accuracies[5][0]=round(accuracy_score(bef_sad_y,predict_sad_b),2)\n",
    "\n",
    "predict_sad_a= model_a.predict(aft_sad_x)\n",
    "np.where(np.argmax(predict_sad_a[:]),1,0)\n",
    "for i in range(len(predict_sad_a)):\n",
    "    temp = np.argmax(predict_sad_a[i])\n",
    "    predict_sad_a[i] = np.zeros((1,7))\n",
    "    predict_sad_a[i][temp] = 1\n",
    "accuracies[5][1]=round(accuracy_score(aft_sad_y,predict_sad_a),2)\n",
    "\n",
    "predict_neutral_b= model_b.predict(bef_neutral_x)\n",
    "np.where(np.argmax(predict_neutral_b[:]),1,0)\n",
    "for i in range(len(predict_neutral_b)):\n",
    "    temp = np.argmax(predict_neutral_b[i])\n",
    "    predict_neutral_b[i] = np.zeros((1,7))\n",
    "    predict_neutral_b[i][temp] = 1\n",
    "accuracies[6][0]=round(accuracy_score(bef_neutral_y,predict_neutral_b),2)\n",
    "\n",
    "predict_neutral_a= model_a.predict(aft_neutral_x)\n",
    "np.where(np.argmax(predict_neutral_a[:]),1,0)\n",
    "for i in range(len(predict_neutral_a)):\n",
    "    temp = np.argmax(predict_neutral_a[i])\n",
    "    predict_neutral_a[i] = np.zeros((1,7))\n",
    "    predict_neutral_a[i][temp] = 1\n",
    "accuracies[6][1]=round(accuracy_score(aft_neutral_y,predict_neutral_a),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry [0.92, 0.84]\n",
      "\n",
      "boredom [0.75, 0.64]\n",
      "\n",
      "disgust [0.67, 0.63]\n",
      "\n",
      "fear [0.75, 0.74]\n",
      "\n",
      "happy [0.63, 0.65]\n",
      "\n",
      "sad [0.97, 0.93]\n",
      "\n",
      "neutral [0.42, 0.81]\n"
     ]
    }
   ],
   "source": [
    "print('angry', (accuracies[0]))\n",
    "print('\\nboredom', accuracies[1])\n",
    "print('\\ndisgust', accuracies[2])\n",
    "print('\\nfear', accuracies[3])\n",
    "print('\\nhappy', accuracies[4])\n",
    "print('\\nsad', accuracies[5])\n",
    "print('\\nneutral', accuracies[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number shows the prediction accuracy of the original file and the one after shows after silence removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the results shows that there is almost no effect on emotions like fear,happy and disgust. \n",
    "\n",
    "But on the other hand, the accuracy on predicting the emotions like angry,boredom and sad is less than that of those when removed the silence. \n",
    "Also, interesting result is that the audio files having _neutral emotion are a lot easier to predict_ when removed silence from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However , we cannot conclude on this dnn model solely. We have to compare them cnn and svm as well to conclude our results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
