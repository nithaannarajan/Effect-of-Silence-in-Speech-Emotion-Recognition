{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import librosa,librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\BTP\n"
     ]
    }
   ],
   "source": [
    "cd F:\\BTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed silences from the audio files with an application.\n",
    "\n",
    "\"bef_rem\" is the original dataset of audiofiles.\n",
    "\n",
    "\"aft_rem\" is the dataset of audio files after the silence is removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bef_rem=os.listdir('DataSets/wav')\n",
    "aft_rem=os.listdir('remov/wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the silence is removed from the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x20fcc63c978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_path_bef = 'Datasets/wav/08a02Na.wav'\n",
    "x , sr = librosa.load(audio_path_bef)\n",
    "#plt.axis([0, 3, -1, 1])\n",
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "plt.subplot(121)\n",
    "librosa.display.waveplot(x, sr=sr)\n",
    "\n",
    "audio_path_aft= 'remov/wav/08a02Na.wav'\n",
    "x , sr = librosa.load(audio_path_aft)\n",
    "#plt.axis([0, 3, -1, 1])\n",
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "plt.subplot(122)\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly form the above plots that the silence is removed within the audiofile.\n",
    "\n",
    "Let's extract mfcc values from each audio file and its emotion and store them.\n",
    "\n",
    "_x stores mfccs and _y stores emotions from each audio files in before and after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_size = 120\n",
    "bef_x = np.zeros((len(bef_rem),mfcc_size))\n",
    "bef_y = np.zeros((len(bef_rem),7),dtype = int)\n",
    "#bef_x stores the audio files' mfcc\n",
    "#bef_y stores emotions\n",
    "\n",
    "code = {\n",
    "    'W':0, #anger\n",
    "    'L':1, #boredom\n",
    "    'E':2, #disgust\n",
    "    'A':3, #fear\n",
    "    'F':4, #happy\n",
    "    'T':5, #sad\n",
    "    'N':6  #neutral\n",
    "}\n",
    "for i in range(len(bef_rem)):\n",
    "    X, sample_rate = librosa.load('DataSets/wav/'+bef_rem[i], res_type='kaiser_fast')\n",
    "    bef_y[i][code[bef_rem[i][5]]] = 1  \n",
    "    bef_x[i] = np.resize(np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=mfcc_size).T,axis=0),(1,mfcc_size))\n",
    "\n",
    "aft_x = np.zeros((len(aft_rem),mfcc_size))\n",
    "aft_y = np.zeros((len(aft_rem),7),dtype = int)\n",
    "\n",
    "#aft_x stores the mfcc after the removal of silences\n",
    "#aft_y stores emotions\n",
    "\n",
    "for i in range(len(aft_rem)):\n",
    "    X, sample_rate = librosa.load('remov/wav/'+aft_rem[i], res_type='kaiser_fast')\n",
    "    aft_y[i][code[aft_rem[i][5]]] = 1\n",
    "    aft_x[i] = np.resize(np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=mfcc_size).T,axis=0),(1,mfcc_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, categorize the datasets of respective emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_indices = list(np.where(bef_y[:,0]==1)[0])\n",
    "bef_angry_x = np.zeros((len(angry_indices),mfcc_size))\n",
    "bef_angry_y = np.zeros((len(angry_indices),7),dtype=int)\n",
    "aft_angry_x = np.zeros((len(angry_indices),mfcc_size))\n",
    "aft_angry_y = np.zeros((len(angry_indices),7),dtype=int)\n",
    "for i in range(len(angry_indices)):\n",
    "    bef_angry_x[i] = np.resize(bef_x[angry_indices[i]],(1,mfcc_size))\n",
    "    bef_angry_y[i] = np.resize(bef_y[angry_indices[i]],(1,7))\n",
    "    aft_angry_x[i] = np.resize(aft_x[angry_indices[i]],(1,mfcc_size))\n",
    "    aft_angry_y[i] = np.resize(aft_y[angry_indices[i]],(1,7))\n",
    "\n",
    "boredom_indices = list(np.where(bef_y[:,1]==1)[0])\n",
    "bef_boredom_x = np.zeros((len(boredom_indices),mfcc_size))\n",
    "bef_boredom_y = np.zeros((len(boredom_indices),7),dtype=int)\n",
    "aft_boredom_x = np.zeros((len(boredom_indices),mfcc_size))\n",
    "aft_boredom_y = np.zeros((len(boredom_indices),7),dtype=int)\n",
    "for i in range(len(boredom_indices)):\n",
    "    bef_boredom_x[i] = np.resize(bef_x[boredom_indices[i]],(1,mfcc_size))\n",
    "    bef_boredom_y[i] = np.resize(bef_y[boredom_indices[i]],(1,7))\n",
    "    aft_boredom_x[i] = np.resize(aft_x[boredom_indices[i]],(1,mfcc_size))\n",
    "    aft_boredom_y[i] = np.resize(aft_y[boredom_indices[i]],(1,7))\n",
    "\n",
    "disgust_indices = list(np.where(bef_y[:,2]==1)[0])\n",
    "bef_disgust_x = np.zeros((len(disgust_indices),mfcc_size))\n",
    "bef_disgust_y = np.zeros((len(disgust_indices),7),dtype=int)\n",
    "aft_disgust_x = np.zeros((len(disgust_indices),mfcc_size))\n",
    "aft_disgust_y = np.zeros((len(disgust_indices),7),dtype=int)\n",
    "for i in range(len(disgust_indices)):\n",
    "    bef_disgust_x[i] = np.resize(bef_x[disgust_indices[i]],(1,mfcc_size))\n",
    "    bef_disgust_y[i] = np.resize(bef_y[disgust_indices[i]],(1,7))\n",
    "    aft_disgust_x[i] = np.resize(aft_x[disgust_indices[i]],(1,mfcc_size))\n",
    "    aft_disgust_y[i] = np.resize(aft_y[disgust_indices[i]],(1,7))\n",
    "\n",
    "fear_indices = list(np.where(bef_y[:,3]==1)[0])\n",
    "bef_fear_x = np.zeros((len(fear_indices),mfcc_size))\n",
    "bef_fear_y = np.zeros((len(fear_indices),7),dtype=int)\n",
    "aft_fear_x = np.zeros((len(fear_indices),mfcc_size))\n",
    "aft_fear_y = np.zeros((len(fear_indices),7),dtype=int)\n",
    "for i in range(len(fear_indices)):\n",
    "    bef_fear_x[i] = np.resize(bef_x[fear_indices[i]],(1,mfcc_size))\n",
    "    bef_fear_y[i] = np.resize(bef_y[fear_indices[i]],(1,7))\n",
    "    aft_fear_x[i] = np.resize(aft_x[fear_indices[i]],(1,mfcc_size))\n",
    "    aft_fear_y[i] = np.resize(aft_y[fear_indices[i]],(1,7))\n",
    "\n",
    "happy_indices = list(np.where(bef_y[:,4]==1)[0])\n",
    "bef_happy_x = np.zeros((len(happy_indices),mfcc_size))\n",
    "bef_happy_y = np.zeros((len(happy_indices),7),dtype=int)\n",
    "aft_happy_x = np.zeros((len(happy_indices),mfcc_size))\n",
    "aft_happy_y = np.zeros((len(happy_indices),7),dtype=int)\n",
    "for i in range(len(happy_indices)):\n",
    "    bef_happy_x[i] = np.resize(bef_x[happy_indices[i]],(1,mfcc_size))\n",
    "    bef_happy_y[i] = np.resize(bef_y[happy_indices[i]],(1,7))\n",
    "    aft_happy_x[i] = np.resize(aft_x[happy_indices[i]],(1,mfcc_size))\n",
    "    aft_happy_y[i] = np.resize(aft_y[happy_indices[i]],(1,7))\n",
    "\n",
    "sad_indices = list(np.where(bef_y[:,5]==1)[0])\n",
    "bef_sad_x = np.zeros((len(sad_indices),mfcc_size))\n",
    "bef_sad_y = np.zeros((len(sad_indices),7),dtype=int)\n",
    "aft_sad_x = np.zeros((len(sad_indices),mfcc_size))\n",
    "aft_sad_y = np.zeros((len(sad_indices),7),dtype=int)\n",
    "for i in range(len(sad_indices)):\n",
    "    bef_sad_x[i] = np.resize(bef_x[sad_indices[i]],(1,mfcc_size))\n",
    "    bef_sad_y[i] = np.resize(bef_y[sad_indices[i]],(1,7))\n",
    "    aft_sad_x[i] = np.resize(aft_x[sad_indices[i]],(1,mfcc_size))\n",
    "    aft_sad_y[i] = np.resize(aft_y[sad_indices[i]],(1,7))\n",
    "\n",
    "neutral_indices = list(np.where(bef_y[:,6]==1)[0])\n",
    "bef_neutral_x = np.zeros((len(neutral_indices),mfcc_size))\n",
    "bef_neutral_y = np.zeros((len(neutral_indices),7),dtype=int)\n",
    "aft_neutral_x = np.zeros((len(neutral_indices),mfcc_size))\n",
    "aft_neutral_y = np.zeros((len(neutral_indices),7),dtype=int)\n",
    "for i in range(len(neutral_indices)):\n",
    "    bef_neutral_x[i] = np.resize(bef_x[neutral_indices[i]],(1,mfcc_size))\n",
    "    bef_neutral_y[i] = np.resize(bef_y[neutral_indices[i]],(1,7))\n",
    "    aft_neutral_x[i] = np.resize(aft_x[neutral_indices[i]],(1,mfcc_size))\n",
    "    aft_neutral_y[i] = np.resize(aft_y[neutral_indices[i]],(1,7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets divide the dataset into test/train/split as 60/20/20 and create a 3 layered dnn model with 128-256-128 nodes.\n",
    "\n",
    "model_a is for the dataset after the removal of silence.\n",
    "\n",
    "model_b is for the dataset before the removal of silemce(original audio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 29.6865 - accuracy: 0.2038 - val_loss: 7.3341 - val_accuracy: 0.3084\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 20.8009 - accuracy: 0.1379 - val_loss: 3.6301 - val_accuracy: 0.2897\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 12.8200 - accuracy: 0.2194 - val_loss: 1.9587 - val_accuracy: 0.2897\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.4192 - accuracy: 0.1755 - val_loss: 1.7339 - val_accuracy: 0.3551\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.5578 - accuracy: 0.2476 - val_loss: 1.7015 - val_accuracy: 0.2897\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.4542 - accuracy: 0.1850 - val_loss: 1.7754 - val_accuracy: 0.2617\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.3192 - accuracy: 0.2539 - val_loss: 1.8028 - val_accuracy: 0.2336\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.9831 - accuracy: 0.2006 - val_loss: 1.8143 - val_accuracy: 0.2243\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.6311 - accuracy: 0.2038 - val_loss: 1.8157 - val_accuracy: 0.2523\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0919 - accuracy: 0.1944 - val_loss: 1.8054 - val_accuracy: 0.2710\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8660 - accuracy: 0.2257 - val_loss: 1.7969 - val_accuracy: 0.2991\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3894 - accuracy: 0.2759 - val_loss: 1.7853 - val_accuracy: 0.3458\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4565 - accuracy: 0.2320 - val_loss: 1.7778 - val_accuracy: 0.3738\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2057 - accuracy: 0.2571 - val_loss: 1.7754 - val_accuracy: 0.3364\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0892 - accuracy: 0.2602 - val_loss: 1.7741 - val_accuracy: 0.3925\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1637 - accuracy: 0.2508 - val_loss: 1.7798 - val_accuracy: 0.3738\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0805 - accuracy: 0.2476 - val_loss: 1.7857 - val_accuracy: 0.3832\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0220 - accuracy: 0.2382 - val_loss: 1.7913 - val_accuracy: 0.3645\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8159 - accuracy: 0.3166 - val_loss: 1.7886 - val_accuracy: 0.3832\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9622 - accuracy: 0.2727 - val_loss: 1.7830 - val_accuracy: 0.4112\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9150 - accuracy: 0.3041 - val_loss: 1.7699 - val_accuracy: 0.4206\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9623 - accuracy: 0.2696 - val_loss: 1.7378 - val_accuracy: 0.4206\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8530 - accuracy: 0.3103 - val_loss: 1.7278 - val_accuracy: 0.4112\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8100 - accuracy: 0.2790 - val_loss: 1.7245 - val_accuracy: 0.3925\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8552 - accuracy: 0.3166 - val_loss: 1.6984 - val_accuracy: 0.3832\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7127 - accuracy: 0.3448 - val_loss: 1.6672 - val_accuracy: 0.4019\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7544 - accuracy: 0.3323 - val_loss: 1.6602 - val_accuracy: 0.4299\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7301 - accuracy: 0.3448 - val_loss: 1.6561 - val_accuracy: 0.4299\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7813 - accuracy: 0.3135 - val_loss: 1.6507 - val_accuracy: 0.4486\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6583 - accuracy: 0.3448 - val_loss: 1.6490 - val_accuracy: 0.4393\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.6985 - accuracy: 0.3323 - val_loss: 1.6207 - val_accuracy: 0.4486\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.6563 - accuracy: 0.3605 - val_loss: 1.5963 - val_accuracy: 0.4486\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7102 - accuracy: 0.3605 - val_loss: 1.6076 - val_accuracy: 0.4766\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5962 - accuracy: 0.3668 - val_loss: 1.5976 - val_accuracy: 0.4579\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6158 - accuracy: 0.3699 - val_loss: 1.5726 - val_accuracy: 0.4579\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5860 - accuracy: 0.3793 - val_loss: 1.5385 - val_accuracy: 0.4486\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5676 - accuracy: 0.3793 - val_loss: 1.5191 - val_accuracy: 0.4393\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5659 - accuracy: 0.3793 - val_loss: 1.4988 - val_accuracy: 0.4486\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5559 - accuracy: 0.3950 - val_loss: 1.5153 - val_accuracy: 0.4299\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5907 - accuracy: 0.4075 - val_loss: 1.5082 - val_accuracy: 0.4860\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6069 - accuracy: 0.3699 - val_loss: 1.4929 - val_accuracy: 0.4766\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5088 - accuracy: 0.3762 - val_loss: 1.4556 - val_accuracy: 0.4766\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4737 - accuracy: 0.4232 - val_loss: 1.4455 - val_accuracy: 0.4953\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5270 - accuracy: 0.3856 - val_loss: 1.4098 - val_accuracy: 0.5047\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5151 - accuracy: 0.4232 - val_loss: 1.4141 - val_accuracy: 0.5234\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4812 - accuracy: 0.4201 - val_loss: 1.4065 - val_accuracy: 0.4766\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4003 - accuracy: 0.4169 - val_loss: 1.3764 - val_accuracy: 0.4486\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.4013 - val_loss: 1.4017 - val_accuracy: 0.4860\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5035 - accuracy: 0.4044 - val_loss: 1.3870 - val_accuracy: 0.5047\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4367 - accuracy: 0.4389 - val_loss: 1.3670 - val_accuracy: 0.4953\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4967 - accuracy: 0.4201 - val_loss: 1.3534 - val_accuracy: 0.4579\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3790 - accuracy: 0.4859 - val_loss: 1.3343 - val_accuracy: 0.4860\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3473 - accuracy: 0.4608 - val_loss: 1.3102 - val_accuracy: 0.4953\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4224 - accuracy: 0.4295 - val_loss: 1.2870 - val_accuracy: 0.4953\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3737 - accuracy: 0.4451 - val_loss: 1.2852 - val_accuracy: 0.4953\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4125 - accuracy: 0.4295 - val_loss: 1.2791 - val_accuracy: 0.4860\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3454 - accuracy: 0.4514 - val_loss: 1.2800 - val_accuracy: 0.5234\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3309 - accuracy: 0.4608 - val_loss: 1.2697 - val_accuracy: 0.5047\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3368 - accuracy: 0.4702 - val_loss: 1.2726 - val_accuracy: 0.5234\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3836 - accuracy: 0.4639 - val_loss: 1.2681 - val_accuracy: 0.5327\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3555 - accuracy: 0.4984 - val_loss: 1.2505 - val_accuracy: 0.5701\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3161 - accuracy: 0.5235 - val_loss: 1.2454 - val_accuracy: 0.5421\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2937 - accuracy: 0.4765 - val_loss: 1.2370 - val_accuracy: 0.5981\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3247 - accuracy: 0.4577 - val_loss: 1.2217 - val_accuracy: 0.5981\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3207 - accuracy: 0.4734 - val_loss: 1.2001 - val_accuracy: 0.5981\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2966 - accuracy: 0.4671 - val_loss: 1.2270 - val_accuracy: 0.5607\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.2872 - accuracy: 0.4859 - val_loss: 1.2141 - val_accuracy: 0.5514\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2376 - accuracy: 0.4922 - val_loss: 1.1738 - val_accuracy: 0.5514\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2969 - accuracy: 0.5298 - val_loss: 1.1935 - val_accuracy: 0.5327\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1983 - accuracy: 0.5486 - val_loss: 1.1955 - val_accuracy: 0.5794\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2597 - accuracy: 0.4953 - val_loss: 1.1742 - val_accuracy: 0.6075\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2192 - accuracy: 0.5298 - val_loss: 1.1412 - val_accuracy: 0.5607\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1755 - accuracy: 0.5549 - val_loss: 1.1461 - val_accuracy: 0.5421\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1713 - accuracy: 0.5361 - val_loss: 1.1619 - val_accuracy: 0.5794\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2400 - accuracy: 0.4765 - val_loss: 1.1829 - val_accuracy: 0.5514\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1473 - accuracy: 0.5392 - val_loss: 1.1551 - val_accuracy: 0.5607\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2043 - accuracy: 0.5298 - val_loss: 1.1426 - val_accuracy: 0.5701\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1637 - accuracy: 0.5078 - val_loss: 1.1307 - val_accuracy: 0.6262\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1869 - accuracy: 0.5235 - val_loss: 1.1407 - val_accuracy: 0.5888\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1952 - accuracy: 0.5486 - val_loss: 1.1054 - val_accuracy: 0.6355\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1821 - accuracy: 0.5549 - val_loss: 1.0987 - val_accuracy: 0.6355\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1750 - accuracy: 0.5361 - val_loss: 1.1126 - val_accuracy: 0.5981\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1176 - accuracy: 0.5517 - val_loss: 1.0937 - val_accuracy: 0.5701\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0676 - accuracy: 0.6019 - val_loss: 1.1092 - val_accuracy: 0.5888\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0992 - accuracy: 0.5768 - val_loss: 1.1106 - val_accuracy: 0.6168\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0782 - accuracy: 0.5329 - val_loss: 1.0652 - val_accuracy: 0.5514\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1307 - accuracy: 0.5705 - val_loss: 1.0741 - val_accuracy: 0.5514\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1245 - accuracy: 0.5172 - val_loss: 1.1342 - val_accuracy: 0.5794\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0983 - accuracy: 0.5611 - val_loss: 1.1177 - val_accuracy: 0.5701\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0981 - accuracy: 0.5549 - val_loss: 1.1085 - val_accuracy: 0.6075\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1254 - accuracy: 0.5674 - val_loss: 1.0885 - val_accuracy: 0.5794\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0974 - accuracy: 0.5768 - val_loss: 1.0674 - val_accuracy: 0.5981\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0472 - accuracy: 0.6113 - val_loss: 1.0710 - val_accuracy: 0.6075\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0888 - accuracy: 0.5580 - val_loss: 1.0826 - val_accuracy: 0.5888\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0727 - accuracy: 0.5674 - val_loss: 1.0792 - val_accuracy: 0.5701\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9980 - accuracy: 0.6301 - val_loss: 1.0782 - val_accuracy: 0.6075\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0430 - accuracy: 0.5580 - val_loss: 1.0647 - val_accuracy: 0.6262\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0335 - accuracy: 0.6113 - val_loss: 1.0837 - val_accuracy: 0.6168\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0669 - accuracy: 0.5517 - val_loss: 1.0313 - val_accuracy: 0.6542\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0357 - accuracy: 0.5768 - val_loss: 1.0451 - val_accuracy: 0.6542\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0018 - accuracy: 0.6113 - val_loss: 1.0663 - val_accuracy: 0.5981\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9867 - accuracy: 0.6238 - val_loss: 1.0731 - val_accuracy: 0.6449\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0037 - accuracy: 0.6082 - val_loss: 1.0549 - val_accuracy: 0.6075\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0054 - accuracy: 0.5674 - val_loss: 1.0344 - val_accuracy: 0.6355\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9292 - accuracy: 0.6332 - val_loss: 1.0909 - val_accuracy: 0.5701\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0351 - accuracy: 0.5893 - val_loss: 1.0820 - val_accuracy: 0.5981\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9446 - accuracy: 0.6270 - val_loss: 1.0960 - val_accuracy: 0.5514\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0140 - accuracy: 0.6270 - val_loss: 1.0784 - val_accuracy: 0.5888\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9799 - accuracy: 0.6082 - val_loss: 1.0497 - val_accuracy: 0.6075\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9752 - accuracy: 0.5987 - val_loss: 1.0485 - val_accuracy: 0.5981\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9343 - accuracy: 0.6458 - val_loss: 1.1129 - val_accuracy: 0.5514\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9782 - accuracy: 0.5925 - val_loss: 1.0963 - val_accuracy: 0.5421\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9690 - accuracy: 0.6270 - val_loss: 1.0774 - val_accuracy: 0.5981\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9069 - accuracy: 0.6458 - val_loss: 1.0489 - val_accuracy: 0.6262\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8879 - accuracy: 0.6583 - val_loss: 1.0446 - val_accuracy: 0.5981\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9223 - accuracy: 0.6552 - val_loss: 1.0903 - val_accuracy: 0.6168\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8747 - accuracy: 0.6458 - val_loss: 1.0681 - val_accuracy: 0.6075\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9019 - accuracy: 0.6113 - val_loss: 1.0230 - val_accuracy: 0.6542\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8827 - accuracy: 0.6238 - val_loss: 1.0698 - val_accuracy: 0.6542\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9203 - accuracy: 0.6520 - val_loss: 1.0679 - val_accuracy: 0.5888\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9203 - accuracy: 0.6426 - val_loss: 1.0556 - val_accuracy: 0.6168\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8934 - accuracy: 0.6489 - val_loss: 1.0377 - val_accuracy: 0.6449\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8695 - accuracy: 0.6238 - val_loss: 1.0078 - val_accuracy: 0.6449\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8915 - accuracy: 0.6395 - val_loss: 1.0715 - val_accuracy: 0.5888\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8499 - accuracy: 0.6583 - val_loss: 1.0791 - val_accuracy: 0.5794\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8351 - accuracy: 0.6897 - val_loss: 1.0500 - val_accuracy: 0.5794\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9078 - accuracy: 0.6364 - val_loss: 1.0403 - val_accuracy: 0.6449\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - accuracy: 0.6583 - val_loss: 1.0210 - val_accuracy: 0.6449\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7547 - accuracy: 0.6959 - val_loss: 1.0718 - val_accuracy: 0.6168\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7878 - accuracy: 0.6646 - val_loss: 1.0857 - val_accuracy: 0.6075\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8045 - accuracy: 0.6834 - val_loss: 1.0129 - val_accuracy: 0.6449\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7759 - accuracy: 0.6834 - val_loss: 1.0741 - val_accuracy: 0.5981\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8240 - accuracy: 0.6740 - val_loss: 1.0534 - val_accuracy: 0.5514\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8431 - accuracy: 0.6458 - val_loss: 1.0284 - val_accuracy: 0.6355\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7846 - accuracy: 0.6959 - val_loss: 0.9569 - val_accuracy: 0.6636\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7649 - accuracy: 0.6803 - val_loss: 1.0024 - val_accuracy: 0.6636\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - accuracy: 0.6771 - val_loss: 1.0643 - val_accuracy: 0.5701\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7760 - accuracy: 0.6865 - val_loss: 1.0671 - val_accuracy: 0.6168\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7975 - accuracy: 0.6771 - val_loss: 1.0623 - val_accuracy: 0.6075\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8325 - accuracy: 0.6740 - val_loss: 1.1582 - val_accuracy: 0.5701\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7406 - accuracy: 0.7085 - val_loss: 1.0287 - val_accuracy: 0.6168\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7696 - accuracy: 0.6740 - val_loss: 1.0286 - val_accuracy: 0.6542\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7672 - accuracy: 0.7147 - val_loss: 1.0731 - val_accuracy: 0.6262\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7546 - accuracy: 0.6677 - val_loss: 1.0739 - val_accuracy: 0.6542\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6959 - accuracy: 0.7179 - val_loss: 0.9514 - val_accuracy: 0.6636\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7119 - accuracy: 0.7116 - val_loss: 1.0077 - val_accuracy: 0.6075\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7079 - accuracy: 0.7053 - val_loss: 1.0400 - val_accuracy: 0.6636\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7645 - accuracy: 0.7210 - val_loss: 0.9897 - val_accuracy: 0.7196\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7410 - accuracy: 0.7116 - val_loss: 0.9844 - val_accuracy: 0.7009\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6651 - accuracy: 0.6991 - val_loss: 1.0180 - val_accuracy: 0.6449\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7332 - accuracy: 0.6928 - val_loss: 1.1073 - val_accuracy: 0.5888\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7091 - accuracy: 0.7241 - val_loss: 1.0438 - val_accuracy: 0.6262\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.7524 - val_loss: 1.0368 - val_accuracy: 0.6636\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7090 - accuracy: 0.7398 - val_loss: 0.9925 - val_accuracy: 0.6449\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6990 - accuracy: 0.7398 - val_loss: 1.1233 - val_accuracy: 0.6449\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6759 - accuracy: 0.7304 - val_loss: 1.0443 - val_accuracy: 0.6449\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7222 - accuracy: 0.7116 - val_loss: 1.1108 - val_accuracy: 0.5888\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7136 - accuracy: 0.7116 - val_loss: 1.0540 - val_accuracy: 0.6729\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7165 - accuracy: 0.7367 - val_loss: 1.0429 - val_accuracy: 0.6355\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.7680 - val_loss: 1.0492 - val_accuracy: 0.6168\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7501 - accuracy: 0.7053 - val_loss: 1.0091 - val_accuracy: 0.6636\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6928 - val_loss: 1.0434 - val_accuracy: 0.6636\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6489 - accuracy: 0.7461 - val_loss: 1.0924 - val_accuracy: 0.7009\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6267 - accuracy: 0.7210 - val_loss: 1.1079 - val_accuracy: 0.6262\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6597 - accuracy: 0.7555 - val_loss: 1.0012 - val_accuracy: 0.6822\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6182 - accuracy: 0.7586 - val_loss: 1.0240 - val_accuracy: 0.7196\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5844 - accuracy: 0.7900 - val_loss: 0.9751 - val_accuracy: 0.7103\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.7304 - val_loss: 1.0716 - val_accuracy: 0.7009\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6032 - accuracy: 0.7712 - val_loss: 1.1081 - val_accuracy: 0.6729\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6470 - accuracy: 0.7429 - val_loss: 1.0457 - val_accuracy: 0.6916\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5973 - accuracy: 0.7962 - val_loss: 1.0350 - val_accuracy: 0.6729\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.7492 - val_loss: 1.1068 - val_accuracy: 0.6168\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.7147 - val_loss: 1.0770 - val_accuracy: 0.6822\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5976 - accuracy: 0.7586 - val_loss: 1.0988 - val_accuracy: 0.6636\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6483 - accuracy: 0.7618 - val_loss: 1.0523 - val_accuracy: 0.6916\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6801 - accuracy: 0.7429 - val_loss: 1.0149 - val_accuracy: 0.6822\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.7461 - val_loss: 1.1195 - val_accuracy: 0.6729\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7279 - accuracy: 0.7335 - val_loss: 1.1209 - val_accuracy: 0.6822\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5942 - accuracy: 0.7367 - val_loss: 1.0764 - val_accuracy: 0.6822\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5763 - accuracy: 0.7837 - val_loss: 1.1767 - val_accuracy: 0.6542\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6207 - accuracy: 0.7429 - val_loss: 1.1990 - val_accuracy: 0.6636\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.7179 - val_loss: 1.0578 - val_accuracy: 0.6542\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5827 - accuracy: 0.7806 - val_loss: 1.1341 - val_accuracy: 0.6822\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5445 - accuracy: 0.7868 - val_loss: 1.1085 - val_accuracy: 0.6729\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6146 - accuracy: 0.7398 - val_loss: 1.0981 - val_accuracy: 0.6449\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6040 - accuracy: 0.7398 - val_loss: 1.0654 - val_accuracy: 0.6916\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6369 - accuracy: 0.7712 - val_loss: 1.0667 - val_accuracy: 0.6729\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.7743 - val_loss: 1.1304 - val_accuracy: 0.6449\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5471 - accuracy: 0.8119 - val_loss: 1.1719 - val_accuracy: 0.6636\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5805 - accuracy: 0.7524 - val_loss: 1.0819 - val_accuracy: 0.7103\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.8119 - val_loss: 1.1336 - val_accuracy: 0.6542\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5926 - accuracy: 0.7586 - val_loss: 1.1714 - val_accuracy: 0.7009\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6463 - accuracy: 0.7555 - val_loss: 0.9879 - val_accuracy: 0.7196\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.8245 - val_loss: 1.1360 - val_accuracy: 0.6636\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5416 - accuracy: 0.8056 - val_loss: 1.1440 - val_accuracy: 0.6822\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5287 - accuracy: 0.7868 - val_loss: 1.1070 - val_accuracy: 0.7196\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.7774 - val_loss: 1.2337 - val_accuracy: 0.6449\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5250 - accuracy: 0.7680 - val_loss: 1.1571 - val_accuracy: 0.6449\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5650 - accuracy: 0.7837 - val_loss: 1.0193 - val_accuracy: 0.7103\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5283 - accuracy: 0.7994 - val_loss: 1.0912 - val_accuracy: 0.7009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20fd656c5f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xa_train1, Xa_test, ya_train1, ya_test = train_test_split(aft_x, aft_y, test_size=0.2, random_state=42)\n",
    "Xa_train, Xa_valid, ya_train, ya_valid = train_test_split(Xa_train1, ya_train1, test_size=0.25, random_state=42)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "num_labels = 7\n",
    "filter_size = 2\n",
    "\n",
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Dense(128, input_shape=(mfcc_size,)))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(256))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(128))\n",
    "model_a.add(Activation('relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(num_labels))\n",
    "model_a.add(Activation('softmax'))\n",
    "\n",
    "model_a.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "\n",
    "model_a.fit(Xa_train, ya_train, batch_size=30, epochs=200, validation_data=(Xa_valid,ya_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of the model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dnn model after removing the silences:  0.6542056074766355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predict_ya = model_a.predict(Xa_test)\n",
    "predicted_train_a = model_a.predict(Xa_train)\n",
    "predicted_test_a = predict_ya\n",
    "np.where(np.argmax(predict_ya[:]),1,0)\n",
    "for i in range(len(predict_ya)):\n",
    "    temp = np.argmax(predict_ya[i])\n",
    "    predict_ya[i] = np.zeros((1,7))\n",
    "    predict_ya[i][temp] = 1\n",
    "aft_acc=accuracy_score(ya_test,predict_ya)\n",
    "print(\"Accuracy of the dnn model after removing the silences: \",aft_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with model_b with the Dataset before silence removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 32.6042 - accuracy: 0.1787 - val_loss: 7.3723 - val_accuracy: 0.2336\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 23.0618 - accuracy: 0.1317 - val_loss: 4.1173 - val_accuracy: 0.2804\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.4172 - accuracy: 0.1693 - val_loss: 2.3863 - val_accuracy: 0.2710\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.4675 - accuracy: 0.1787 - val_loss: 1.8390 - val_accuracy: 0.2710\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.2800 - accuracy: 0.1850 - val_loss: 1.7559 - val_accuracy: 0.2804\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.5840 - accuracy: 0.1411 - val_loss: 1.7987 - val_accuracy: 0.3084\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.6620 - accuracy: 0.1881 - val_loss: 1.7951 - val_accuracy: 0.2430\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.7073 - accuracy: 0.1881 - val_loss: 1.8048 - val_accuracy: 0.2243\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0642 - accuracy: 0.2006 - val_loss: 1.7946 - val_accuracy: 0.2804\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.4292 - accuracy: 0.1944 - val_loss: 1.7846 - val_accuracy: 0.3645\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9948 - accuracy: 0.2069 - val_loss: 1.7935 - val_accuracy: 0.3738\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8270 - accuracy: 0.2508 - val_loss: 1.7907 - val_accuracy: 0.3271\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.7025 - accuracy: 0.2226 - val_loss: 1.7909 - val_accuracy: 0.3084\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3541 - accuracy: 0.2633 - val_loss: 1.7948 - val_accuracy: 0.2897\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2759 - accuracy: 0.2790 - val_loss: 1.8082 - val_accuracy: 0.3084\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2099 - accuracy: 0.2633 - val_loss: 1.7922 - val_accuracy: 0.2804\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1041 - accuracy: 0.2884 - val_loss: 1.7783 - val_accuracy: 0.2991\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1017 - accuracy: 0.2602 - val_loss: 1.7968 - val_accuracy: 0.2897\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8897 - accuracy: 0.3292 - val_loss: 1.7990 - val_accuracy: 0.3364\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9959 - accuracy: 0.2665 - val_loss: 1.7885 - val_accuracy: 0.3645\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9721 - accuracy: 0.2727 - val_loss: 1.7753 - val_accuracy: 0.3645\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9244 - accuracy: 0.2727 - val_loss: 1.7441 - val_accuracy: 0.3551\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8181 - accuracy: 0.3229 - val_loss: 1.7013 - val_accuracy: 0.2991\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.7799 - accuracy: 0.3354 - val_loss: 1.7109 - val_accuracy: 0.3178\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8851 - accuracy: 0.2727 - val_loss: 1.7239 - val_accuracy: 0.3084\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8087 - accuracy: 0.2915 - val_loss: 1.7084 - val_accuracy: 0.3271\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.7874 - accuracy: 0.3135 - val_loss: 1.6841 - val_accuracy: 0.4019\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7231 - accuracy: 0.3260 - val_loss: 1.6712 - val_accuracy: 0.3925\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7850 - accuracy: 0.2759 - val_loss: 1.6575 - val_accuracy: 0.4019\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6805 - accuracy: 0.3574 - val_loss: 1.6370 - val_accuracy: 0.4393\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7075 - accuracy: 0.3135 - val_loss: 1.5899 - val_accuracy: 0.4206\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5962 - accuracy: 0.3981 - val_loss: 1.5682 - val_accuracy: 0.4112\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6355 - accuracy: 0.3480 - val_loss: 1.5561 - val_accuracy: 0.4206\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6145 - accuracy: 0.3574 - val_loss: 1.5375 - val_accuracy: 0.4579\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6236 - accuracy: 0.3793 - val_loss: 1.5501 - val_accuracy: 0.4299\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6306 - accuracy: 0.3636 - val_loss: 1.5288 - val_accuracy: 0.4579\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5929 - accuracy: 0.3918 - val_loss: 1.5150 - val_accuracy: 0.4393\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5596 - accuracy: 0.3856 - val_loss: 1.4893 - val_accuracy: 0.4579\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4974 - accuracy: 0.4232 - val_loss: 1.4702 - val_accuracy: 0.5140\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5365 - accuracy: 0.3887 - val_loss: 1.4849 - val_accuracy: 0.5047\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5498 - accuracy: 0.3386 - val_loss: 1.4535 - val_accuracy: 0.5140\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5095 - accuracy: 0.3887 - val_loss: 1.4166 - val_accuracy: 0.4953\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4796 - accuracy: 0.4201 - val_loss: 1.3996 - val_accuracy: 0.4860\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5286 - accuracy: 0.4169 - val_loss: 1.3999 - val_accuracy: 0.4766\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4893 - accuracy: 0.4326 - val_loss: 1.3829 - val_accuracy: 0.5234\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3890 - accuracy: 0.4514 - val_loss: 1.3501 - val_accuracy: 0.4953\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4537 - accuracy: 0.4483 - val_loss: 1.3464 - val_accuracy: 0.5327\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4981 - accuracy: 0.3856 - val_loss: 1.3417 - val_accuracy: 0.5701\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4125 - accuracy: 0.4420 - val_loss: 1.3340 - val_accuracy: 0.5421\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3807 - accuracy: 0.4451 - val_loss: 1.3413 - val_accuracy: 0.5421\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4197 - accuracy: 0.4451 - val_loss: 1.3055 - val_accuracy: 0.5514\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3640 - accuracy: 0.4420 - val_loss: 1.3083 - val_accuracy: 0.5607\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3589 - accuracy: 0.4890 - val_loss: 1.3052 - val_accuracy: 0.5701\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3989 - accuracy: 0.4545 - val_loss: 1.2749 - val_accuracy: 0.6075\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3141 - accuracy: 0.4984 - val_loss: 1.2691 - val_accuracy: 0.6168\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3302 - accuracy: 0.4796 - val_loss: 1.2616 - val_accuracy: 0.5701\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3171 - accuracy: 0.4577 - val_loss: 1.2561 - val_accuracy: 0.5607\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3328 - accuracy: 0.4734 - val_loss: 1.2329 - val_accuracy: 0.5981\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3001 - accuracy: 0.5016 - val_loss: 1.2107 - val_accuracy: 0.6075\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2531 - accuracy: 0.4796 - val_loss: 1.1852 - val_accuracy: 0.6075\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2006 - accuracy: 0.5266 - val_loss: 1.1756 - val_accuracy: 0.6168\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2575 - accuracy: 0.5110 - val_loss: 1.1515 - val_accuracy: 0.6262\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1967 - accuracy: 0.4671 - val_loss: 1.1580 - val_accuracy: 0.6449\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2518 - accuracy: 0.5078 - val_loss: 1.1475 - val_accuracy: 0.6636\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2333 - accuracy: 0.5047 - val_loss: 1.1148 - val_accuracy: 0.6636\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2351 - accuracy: 0.4859 - val_loss: 1.1228 - val_accuracy: 0.6729\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2739 - accuracy: 0.4953 - val_loss: 1.1420 - val_accuracy: 0.6542\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1846 - accuracy: 0.5266 - val_loss: 1.1354 - val_accuracy: 0.6262\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2324 - accuracy: 0.5110 - val_loss: 1.1075 - val_accuracy: 0.6729\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1145 - accuracy: 0.5392 - val_loss: 1.0916 - val_accuracy: 0.6355\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1420 - accuracy: 0.5172 - val_loss: 1.0886 - val_accuracy: 0.5981\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1455 - accuracy: 0.5266 - val_loss: 1.0870 - val_accuracy: 0.6075\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1602 - accuracy: 0.5517 - val_loss: 1.0921 - val_accuracy: 0.6355\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0904 - accuracy: 0.5925 - val_loss: 1.0635 - val_accuracy: 0.6822\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1818 - accuracy: 0.5517 - val_loss: 1.0257 - val_accuracy: 0.7103\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1154 - accuracy: 0.5423 - val_loss: 1.0552 - val_accuracy: 0.6355\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1290 - accuracy: 0.5172 - val_loss: 1.0366 - val_accuracy: 0.6168\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1069 - accuracy: 0.5705 - val_loss: 1.0232 - val_accuracy: 0.6916\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1172 - accuracy: 0.5392 - val_loss: 1.0389 - val_accuracy: 0.7009\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0278 - accuracy: 0.5831 - val_loss: 0.9999 - val_accuracy: 0.7290\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1275 - accuracy: 0.5455 - val_loss: 0.9620 - val_accuracy: 0.7196\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1589 - accuracy: 0.5423 - val_loss: 0.9739 - val_accuracy: 0.6822\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0874 - accuracy: 0.5705 - val_loss: 0.9692 - val_accuracy: 0.7009\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0856 - accuracy: 0.5611 - val_loss: 1.0072 - val_accuracy: 0.6822\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9953 - accuracy: 0.5956 - val_loss: 0.9613 - val_accuracy: 0.7103\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0016 - accuracy: 0.6364 - val_loss: 0.9793 - val_accuracy: 0.7196\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9505 - accuracy: 0.6395 - val_loss: 0.9549 - val_accuracy: 0.6916\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0261 - accuracy: 0.5611 - val_loss: 0.9557 - val_accuracy: 0.6916\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9777 - accuracy: 0.6113 - val_loss: 0.9386 - val_accuracy: 0.6636\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9804 - accuracy: 0.6301 - val_loss: 0.9756 - val_accuracy: 0.7290\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9817 - accuracy: 0.6019 - val_loss: 0.9687 - val_accuracy: 0.6542\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9252 - accuracy: 0.6364 - val_loss: 0.9762 - val_accuracy: 0.6262\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9697 - accuracy: 0.6395 - val_loss: 0.9427 - val_accuracy: 0.6636\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9225 - accuracy: 0.6301 - val_loss: 0.9818 - val_accuracy: 0.6168\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9984 - accuracy: 0.5956 - val_loss: 0.9641 - val_accuracy: 0.6822\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8695 - accuracy: 0.6552 - val_loss: 0.9386 - val_accuracy: 0.6636\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8720 - accuracy: 0.6301 - val_loss: 0.8950 - val_accuracy: 0.6729\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9368 - accuracy: 0.6207 - val_loss: 0.8984 - val_accuracy: 0.7664\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8683 - accuracy: 0.6144 - val_loss: 0.8973 - val_accuracy: 0.7290\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9410 - accuracy: 0.6270 - val_loss: 0.9046 - val_accuracy: 0.7383\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8795 - accuracy: 0.6583 - val_loss: 0.9441 - val_accuracy: 0.6636\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8980 - accuracy: 0.6583 - val_loss: 0.9436 - val_accuracy: 0.7290\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8998 - accuracy: 0.6301 - val_loss: 0.9642 - val_accuracy: 0.6262\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - accuracy: 0.6583 - val_loss: 0.9138 - val_accuracy: 0.7009\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8586 - accuracy: 0.6740 - val_loss: 0.8966 - val_accuracy: 0.6729\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8340 - accuracy: 0.6771 - val_loss: 0.8800 - val_accuracy: 0.7290\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8166 - accuracy: 0.7022 - val_loss: 0.9426 - val_accuracy: 0.6916\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9139 - accuracy: 0.6176 - val_loss: 0.8828 - val_accuracy: 0.6822\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8567 - accuracy: 0.6834 - val_loss: 0.9127 - val_accuracy: 0.6449\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - accuracy: 0.6207 - val_loss: 0.9982 - val_accuracy: 0.6729\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - accuracy: 0.6395 - val_loss: 1.0127 - val_accuracy: 0.6729\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7707 - accuracy: 0.7116 - val_loss: 0.8848 - val_accuracy: 0.6822\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8337 - accuracy: 0.6614 - val_loss: 0.8946 - val_accuracy: 0.6636\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7536 - accuracy: 0.6959 - val_loss: 0.9554 - val_accuracy: 0.6729\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7741 - accuracy: 0.6959 - val_loss: 0.8821 - val_accuracy: 0.6822\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7417 - accuracy: 0.6928 - val_loss: 0.8750 - val_accuracy: 0.6729\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7338 - accuracy: 0.7022 - val_loss: 0.8464 - val_accuracy: 0.6916\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8468 - accuracy: 0.6614 - val_loss: 0.8766 - val_accuracy: 0.7196\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7087 - accuracy: 0.7429 - val_loss: 0.9181 - val_accuracy: 0.7196\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7499 - accuracy: 0.6897 - val_loss: 0.8030 - val_accuracy: 0.7477\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7859 - accuracy: 0.6865 - val_loss: 0.8595 - val_accuracy: 0.7383\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7064 - accuracy: 0.7273 - val_loss: 0.8735 - val_accuracy: 0.7103\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7614 - accuracy: 0.7116 - val_loss: 0.8635 - val_accuracy: 0.7570\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7483 - accuracy: 0.7053 - val_loss: 0.8378 - val_accuracy: 0.7103\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7170 - accuracy: 0.7147 - val_loss: 0.9064 - val_accuracy: 0.6636\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7637 - accuracy: 0.6771 - val_loss: 0.8655 - val_accuracy: 0.7477\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7548 - accuracy: 0.6771 - val_loss: 0.9271 - val_accuracy: 0.7196\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7125 - accuracy: 0.7555 - val_loss: 0.8765 - val_accuracy: 0.6916\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6558 - accuracy: 0.7304 - val_loss: 1.0114 - val_accuracy: 0.6729\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6538 - accuracy: 0.7555 - val_loss: 0.8858 - val_accuracy: 0.7196\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7208 - accuracy: 0.7461 - val_loss: 0.8370 - val_accuracy: 0.7383\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7482 - accuracy: 0.6771 - val_loss: 0.8812 - val_accuracy: 0.7196\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6976 - accuracy: 0.7335 - val_loss: 0.8250 - val_accuracy: 0.7290\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.7492 - val_loss: 0.8606 - val_accuracy: 0.7664\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6241 - accuracy: 0.7649 - val_loss: 0.9076 - val_accuracy: 0.7196\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7278 - accuracy: 0.6991 - val_loss: 0.9851 - val_accuracy: 0.7196\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6159 - accuracy: 0.7586 - val_loss: 0.9060 - val_accuracy: 0.7570\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6304 - accuracy: 0.7335 - val_loss: 0.7818 - val_accuracy: 0.7570\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.7618 - val_loss: 0.8758 - val_accuracy: 0.7196\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6638 - accuracy: 0.7147 - val_loss: 0.9065 - val_accuracy: 0.7103\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.7492 - val_loss: 1.0059 - val_accuracy: 0.6822\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6578 - accuracy: 0.6991 - val_loss: 0.9281 - val_accuracy: 0.7103\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6315 - accuracy: 0.7492 - val_loss: 0.8812 - val_accuracy: 0.7570\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6160 - accuracy: 0.7429 - val_loss: 0.9722 - val_accuracy: 0.7477\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.7179 - val_loss: 0.9146 - val_accuracy: 0.7196\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5993 - accuracy: 0.7555 - val_loss: 0.9317 - val_accuracy: 0.7009\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6026 - accuracy: 0.7712 - val_loss: 0.8701 - val_accuracy: 0.7103\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6021 - accuracy: 0.7524 - val_loss: 0.8964 - val_accuracy: 0.7477\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5956 - accuracy: 0.7367 - val_loss: 0.8664 - val_accuracy: 0.7477\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5991 - accuracy: 0.7806 - val_loss: 0.8406 - val_accuracy: 0.7570\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.7367 - val_loss: 0.8620 - val_accuracy: 0.7477\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5348 - accuracy: 0.8025 - val_loss: 0.9139 - val_accuracy: 0.7196\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.6362 - accuracy: 0.7586 - val_loss: 0.8402 - val_accuracy: 0.7570\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5990 - accuracy: 0.7649 - val_loss: 0.7843 - val_accuracy: 0.7570\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5629 - accuracy: 0.7618 - val_loss: 0.9068 - val_accuracy: 0.7383\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 0.7680 - val_loss: 0.9984 - val_accuracy: 0.7103\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5979 - accuracy: 0.7837 - val_loss: 0.8953 - val_accuracy: 0.7570\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5841 - accuracy: 0.7774 - val_loss: 0.8333 - val_accuracy: 0.7757\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5775 - accuracy: 0.7774 - val_loss: 0.9445 - val_accuracy: 0.7103\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5643 - accuracy: 0.7837 - val_loss: 0.9949 - val_accuracy: 0.7477\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6544 - accuracy: 0.7492 - val_loss: 0.9833 - val_accuracy: 0.7196\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6106 - accuracy: 0.7367 - val_loss: 0.9605 - val_accuracy: 0.7290\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6118 - accuracy: 0.7555 - val_loss: 0.9057 - val_accuracy: 0.7664\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5509 - accuracy: 0.7712 - val_loss: 0.9370 - val_accuracy: 0.7757\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7994 - val_loss: 0.8746 - val_accuracy: 0.7757\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4711 - accuracy: 0.8276 - val_loss: 0.8986 - val_accuracy: 0.7664\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4944 - accuracy: 0.8025 - val_loss: 1.0075 - val_accuracy: 0.7290\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.8025 - val_loss: 0.9789 - val_accuracy: 0.7477\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4833 - accuracy: 0.8245 - val_loss: 0.9484 - val_accuracy: 0.7196\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5250 - accuracy: 0.7931 - val_loss: 1.0834 - val_accuracy: 0.7383\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.6146 - accuracy: 0.7680 - val_loss: 0.9888 - val_accuracy: 0.7196\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.5263 - accuracy: 0.7868 - val_loss: 0.9407 - val_accuracy: 0.7383\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5712 - accuracy: 0.7618 - val_loss: 1.0047 - val_accuracy: 0.7477\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5178 - accuracy: 0.7900 - val_loss: 0.8248 - val_accuracy: 0.7757\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5338 - accuracy: 0.7900 - val_loss: 0.9087 - val_accuracy: 0.7944\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5854 - accuracy: 0.7461 - val_loss: 0.9263 - val_accuracy: 0.7757\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5624 - accuracy: 0.7743 - val_loss: 0.9182 - val_accuracy: 0.6916\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5121 - accuracy: 0.8056 - val_loss: 0.8018 - val_accuracy: 0.7664\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5471 - accuracy: 0.7806 - val_loss: 1.0016 - val_accuracy: 0.7009\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.7586 - val_loss: 1.0492 - val_accuracy: 0.7196\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4864 - accuracy: 0.8119 - val_loss: 0.9168 - val_accuracy: 0.7664\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5339 - accuracy: 0.7806 - val_loss: 0.9574 - val_accuracy: 0.7383\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.7962 - val_loss: 1.0429 - val_accuracy: 0.7196\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5352 - accuracy: 0.7931 - val_loss: 0.9162 - val_accuracy: 0.7477\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5113 - accuracy: 0.7868 - val_loss: 0.8910 - val_accuracy: 0.7570\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5073 - accuracy: 0.8119 - val_loss: 1.1160 - val_accuracy: 0.7103\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5062 - accuracy: 0.8119 - val_loss: 0.9502 - val_accuracy: 0.7383\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4566 - accuracy: 0.8276 - val_loss: 1.2071 - val_accuracy: 0.7477\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5275 - accuracy: 0.8025 - val_loss: 0.9625 - val_accuracy: 0.7570\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4967 - accuracy: 0.8119 - val_loss: 0.8727 - val_accuracy: 0.7570\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.8119 - val_loss: 0.9901 - val_accuracy: 0.7850\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5304 - accuracy: 0.8119 - val_loss: 0.8929 - val_accuracy: 0.7757\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4465 - accuracy: 0.8558 - val_loss: 0.9631 - val_accuracy: 0.7757\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4350 - accuracy: 0.8119 - val_loss: 1.1854 - val_accuracy: 0.7570\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5057 - accuracy: 0.8025 - val_loss: 0.9686 - val_accuracy: 0.7570\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5340 - accuracy: 0.7962 - val_loss: 0.9778 - val_accuracy: 0.7757\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4667 - accuracy: 0.8433 - val_loss: 1.0505 - val_accuracy: 0.7477\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.8307 - val_loss: 0.9545 - val_accuracy: 0.7383\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.8213 - val_loss: 1.0149 - val_accuracy: 0.7850\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3530 - accuracy: 0.8683 - val_loss: 1.0809 - val_accuracy: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20fd77a50b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb_train1, Xb_test, yb_train1, yb_test = train_test_split(bef_x, bef_y, test_size=0.2, random_state=42)\n",
    "\n",
    "Xb_train, Xb_valid, yb_train, yb_valid = train_test_split(Xb_train1, yb_train1, test_size=0.25, random_state=42)\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "\n",
    "num_labels = 7\n",
    "filter_size = 2\n",
    "\n",
    "model_b = Sequential()\n",
    "\n",
    "model_b.add(Dense(128, input_shape=(mfcc_size,)))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(256))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(128))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "\n",
    "model_b.add(Dense(num_labels))\n",
    "model_b.add(Activation('softmax'))\n",
    "\n",
    "model_b.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model_b.fit(Xb_train, yb_train, batch_size=30, epochs=200, validation_data=(Xb_valid,yb_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dnn model after removing the silences:  0.6728971962616822\n"
     ]
    }
   ],
   "source": [
    "predict_yb= model_b.predict(Xb_test)\n",
    "predicted_train_b = model_b.predict(Xb_train)\n",
    "predicted_test_b = predict_yb\n",
    "np.where(np.argmax(predict_yb[:]),1,0)\n",
    "for i in range(len(predict_yb)):\n",
    "    temp = np.argmax(predict_yb[i])\n",
    "    \n",
    "    predict_yb[i] = np.zeros((1,7))\n",
    "    predict_yb[i][temp] = 1\n",
    "bef_acc=accuracy_score(yb_test,predict_yb)\n",
    "print(\"Accuracy of the dnn model after removing the silences: \",bef_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here comes the main part . Calculating the accuracies of predicting different emotions before and after silence removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_accuracies = [[0 for i in range(2)] for j in range(7)] \n",
    "\n",
    "predict_angry_b= model_b.predict(bef_angry_x)\n",
    "np.where(np.argmax(predict_angry_b[:]),1,0)\n",
    "for i in range(len(predict_angry_b)):\n",
    "    temp = np.argmax(predict_angry_b[i])\n",
    "    predict_angry_b[i] = np.zeros((1,7))\n",
    "    predict_angry_b[i][temp] = 1\n",
    "dnn_accuracies[0][0]=round((accuracy_score(bef_angry_y,predict_angry_b)),2)\n",
    "\n",
    "predict_angry_a= model_a.predict(aft_angry_x)\n",
    "np.where(np.argmax(predict_angry_a[:]),1,0)\n",
    "for i in range(len(predict_angry_a)):\n",
    "    temp = np.argmax(predict_angry_a[i])\n",
    "    predict_angry_a[i] = np.zeros((1,7))\n",
    "    predict_angry_a[i][temp] = 1\n",
    "dnn_accuracies[0][1]=round(accuracy_score(aft_angry_y,predict_angry_a),2)\n",
    "\n",
    "predict_boredom_b= model_b.predict(bef_boredom_x)\n",
    "np.where(np.argmax(predict_boredom_b[:]),1,0)\n",
    "for i in range(len(predict_boredom_b)):\n",
    "    temp = np.argmax(predict_boredom_b[i])\n",
    "    predict_boredom_b[i] = np.zeros((1,7))\n",
    "    predict_boredom_b[i][temp] = 1\n",
    "dnn_accuracies[1][0]=round(accuracy_score(bef_boredom_y,predict_boredom_b),2)\n",
    "\n",
    "predict_boredom_a= model_a.predict(aft_boredom_x)\n",
    "np.where(np.argmax(predict_boredom_a[:]),1,0)\n",
    "for i in range(len(predict_boredom_a)):\n",
    "    temp = np.argmax(predict_boredom_a[i])\n",
    "    predict_boredom_a[i] = np.zeros((1,7))\n",
    "    predict_boredom_a[i][temp] = 1\n",
    "dnn_accuracies[1][1]=round(accuracy_score(aft_boredom_y,predict_boredom_a),2)\n",
    "\n",
    "predict_disgust_b= model_b.predict(bef_disgust_x)\n",
    "np.where(np.argmax(predict_disgust_b[:]),1,0)\n",
    "for i in range(len(predict_disgust_b)):\n",
    "    temp = np.argmax(predict_disgust_b[i])\n",
    "    predict_disgust_b[i] = np.zeros((1,7))\n",
    "    predict_disgust_b[i][temp] = 1\n",
    "dnn_accuracies[2][0]=round(accuracy_score(bef_disgust_y,predict_disgust_b),2)\n",
    "\n",
    "predict_disgust_a= model_a.predict(aft_disgust_x)\n",
    "np.where(np.argmax(predict_disgust_a[:]),1,0)\n",
    "for i in range(len(predict_disgust_a)):\n",
    "    temp = np.argmax(predict_disgust_a[i])\n",
    "    predict_disgust_a[i] = np.zeros((1,7))\n",
    "    predict_disgust_a[i][temp] = 1\n",
    "dnn_accuracies[2][1]=round(accuracy_score(aft_disgust_y,predict_disgust_a),2)\n",
    "\n",
    "predict_fear_b= model_b.predict(bef_fear_x)\n",
    "np.where(np.argmax(predict_fear_b[:]),1,0)\n",
    "for i in range(len(predict_fear_b)):\n",
    "    temp = np.argmax(predict_fear_b[i])\n",
    "    predict_fear_b[i] = np.zeros((1,7))\n",
    "    predict_fear_b[i][temp] = 1\n",
    "dnn_accuracies[3][0]=round(accuracy_score(bef_fear_y,predict_fear_b),2)\n",
    "\n",
    "predict_fear_a= model_a.predict(aft_fear_x)\n",
    "np.where(np.argmax(predict_fear_a[:]),1,0)\n",
    "for i in range(len(predict_fear_a)):\n",
    "    temp = np.argmax(predict_fear_a[i])\n",
    "    predict_fear_a[i] = np.zeros((1,7))\n",
    "    predict_fear_a[i][temp] = 1\n",
    "dnn_accuracies[3][1]=round(accuracy_score(aft_fear_y,predict_fear_a),2)\n",
    "\n",
    "predict_happy_b= model_b.predict(bef_happy_x)\n",
    "np.where(np.argmax(predict_happy_b[:]),1,0)\n",
    "for i in range(len(predict_happy_b)):\n",
    "    temp = np.argmax(predict_happy_b[i])\n",
    "    predict_happy_b[i] = np.zeros((1,7))\n",
    "    predict_happy_b[i][temp] = 1\n",
    "dnn_accuracies[4][0]=round(accuracy_score(bef_happy_y,predict_happy_b),2)\n",
    "\n",
    "predict_happy_a= model_a.predict(aft_happy_x)\n",
    "np.where(np.argmax(predict_happy_a[:]),1,0)\n",
    "for i in range(len(predict_happy_a)):\n",
    "    temp = np.argmax(predict_happy_a[i])\n",
    "    predict_happy_a[i] = np.zeros((1,7))\n",
    "    predict_happy_a[i][temp] = 1\n",
    "dnn_accuracies[4][1]=round(accuracy_score(aft_happy_y,predict_happy_a),2)\n",
    "\n",
    "predict_sad_b= model_b.predict(bef_sad_x)\n",
    "np.where(np.argmax(predict_sad_b[:]),1,0)\n",
    "for i in range(len(predict_sad_b)):\n",
    "    temp = np.argmax(predict_sad_b[i])\n",
    "    predict_sad_b[i] = np.zeros((1,7))\n",
    "    predict_sad_b[i][temp] = 1\n",
    "dnn_accuracies[5][0]=round(accuracy_score(bef_sad_y,predict_sad_b),2)\n",
    "\n",
    "predict_sad_a= model_a.predict(aft_sad_x)\n",
    "np.where(np.argmax(predict_sad_a[:]),1,0)\n",
    "for i in range(len(predict_sad_a)):\n",
    "    temp = np.argmax(predict_sad_a[i])\n",
    "    predict_sad_a[i] = np.zeros((1,7))\n",
    "    predict_sad_a[i][temp] = 1\n",
    "dnn_accuracies[5][1]=round(accuracy_score(aft_sad_y,predict_sad_a),2)\n",
    "\n",
    "predict_neutral_b= model_b.predict(bef_neutral_x)\n",
    "np.where(np.argmax(predict_neutral_b[:]),1,0)\n",
    "for i in range(len(predict_neutral_b)):\n",
    "    temp = np.argmax(predict_neutral_b[i])\n",
    "    predict_neutral_b[i] = np.zeros((1,7))\n",
    "    predict_neutral_b[i][temp] = 1\n",
    "dnn_accuracies[6][0]=round(accuracy_score(bef_neutral_y,predict_neutral_b),2)\n",
    "\n",
    "predict_neutral_a= model_a.predict(aft_neutral_x)\n",
    "np.where(np.argmax(predict_neutral_a[:]),1,0)\n",
    "for i in range(len(predict_neutral_a)):\n",
    "    temp = np.argmax(predict_neutral_a[i])\n",
    "    predict_neutral_a[i] = np.zeros((1,7))\n",
    "    predict_neutral_a[i][temp] = 1\n",
    "dnn_accuracies[6][1]=round(accuracy_score(aft_neutral_y,predict_neutral_a),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The format is emotion[before_removal   after_removal]\n",
      "\n",
      "angry [0.95, 0.96]\n",
      "\n",
      "boredom [0.75, 0.77]\n",
      "\n",
      "disgust [0.87, 0.72]\n",
      "\n",
      "fear [0.9, 0.8]\n",
      "\n",
      "happy [0.8, 0.77]\n",
      "\n",
      "sad [0.98, 0.97]\n",
      "\n",
      "neutral [0.86, 0.81]\n"
     ]
    }
   ],
   "source": [
    "print(\"The format is emotion[before_removal   after_removal]\")\n",
    "print('\\nangry', (dnn_accuracies[0]))\n",
    "print('\\nboredom', dnn_accuracies[1])\n",
    "print('\\ndisgust', dnn_accuracies[2])\n",
    "print('\\nfear', dnn_accuracies[3])\n",
    "print('\\nhappy', dnn_accuracies[4])\n",
    "print('\\nsad', dnn_accuracies[5])\n",
    "print('\\nneutral', dnn_accuracies[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results i.e the prediction accuracies of different emotions with the dnn model built,\n",
    "\n",
    "We can see that the overall accuracy in predicting the emotion is **decreased when we removed the silence** from the audio file.\n",
    "\n",
    "But on the other hand, the accuracy on predicting the emotions **angry and boredom is less than that of those when removed the silence.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However , we cannot conclude on this dnn model solely. We have to compare with svm as also to conclude our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC doesn't accept 2-d values for y. So, we're going to convert the onehot encoded data into 7 different outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]]\n",
      "tf.Tensor([6 6 5 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "yb_train_emotions = tf.argmax(yb_train1, axis=1)\n",
    "yb_test_emotions = tf.argmax(yb_test, axis=1)\n",
    "ya_train_emotions = tf.argmax(ya_train1, axis=1)\n",
    "ya_test_emotions = tf.argmax(ya_test, axis=1)\n",
    "bef_angry_emotions = tf.argmax(bef_angry_y, axis=1)\n",
    "aft_angry_emotions = tf.argmax(aft_angry_y, axis=1)\n",
    "bef_boredom_emotions = tf.argmax(bef_boredom_y, axis=1)\n",
    "aft_boredom_emotions = tf.argmax(aft_boredom_y, axis=1)\n",
    "bef_disgust_emotions = tf.argmax(bef_disgust_y, axis=1)\n",
    "aft_disgust_emotions = tf.argmax(aft_disgust_y, axis=1)\n",
    "bef_fear_emotions = tf.argmax(bef_fear_y, axis=1)\n",
    "aft_fear_emotions = tf.argmax(aft_fear_y, axis=1)\n",
    "bef_happy_emotions = tf.argmax(bef_happy_y, axis=1)\n",
    "aft_happy_emotions = tf.argmax(aft_happy_y, axis=1)\n",
    "bef_sad_emotions = tf.argmax(bef_sad_y, axis=1)\n",
    "aft_sad_emotions = tf.argmax(aft_sad_y, axis=1)\n",
    "bef_neutral_emotions = tf.argmax(bef_neutral_y, axis=1)\n",
    "aft_neutral_emotions = tf.argmax(aft_neutral_y, axis=1)\n",
    "\n",
    "\n",
    "#lets see what we did here\n",
    "\n",
    "print(yb_train1[3:8])\n",
    "print(yb_train_emotions[3:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf_b is the classifier for the ones before removal of silence.\n",
    "\n",
    "clf_a is the classifier after the removal of silence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 6 5 5 5 5 5 6 4 0 1 1 2 5 3 0 1 0 1 4 0 0 6 5 3 1 6 0 1 4 6 4 0 5 3 4\n",
      " 6 3 4 6 0 2 4 6 0 4 4 3 1 0 0 1 0 6 6 4 0 0 0 0 0 0 1 3 1 3 5 0 6 3 6 4 1\n",
      " 0 1 4 4 1 0 4 3 6 6 6 4 0 1 1 0 5 0 2 2 3 6 6 0 0 6 0 1 6 1 0 0 0] tf.Tensor(\n",
      "[0 0 6 5 5 5 1 1 6 4 0 1 1 0 5 3 0 1 0 1 4 0 0 6 5 6 1 6 4 1 4 6 4 0 5 3 4\n",
      " 1 0 3 1 3 4 4 1 4 4 4 3 2 0 0 1 0 6 6 3 4 0 4 3 0 0 1 4 1 3 5 3 6 3 1 4 1\n",
      " 0 1 4 4 6 0 4 4 6 1 1 4 0 6 6 0 3 0 2 2 4 6 1 0 0 6 0 1 6 6 0 0 0], shape=(107,), dtype=int64)\n",
      "Accuracy: 0.7102803738317757\n"
     ]
    }
   ],
   "source": [
    "clf_b = svm.SVC(kernel='linear')\n",
    "clf_b.fit(Xb_train1, yb_train_emotions)\n",
    "yb_pred = clf_b.predict(Xb_test)\n",
    "print(yb_pred,yb_test_emotions)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(yb_test_emotions,yb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 6 5 5 5 5 5 6 4 0 1 1 2 5 3 0 1 0 1 4 0 0 6 5 3 1 6 0 1 4 6 4 0 5 3 4\n",
      " 6 3 4 6 0 2 4 6 0 4 4 3 1 0 0 1 0 6 6 4 0 0 0 0 0 0 1 3 1 3 5 0 6 3 6 4 1\n",
      " 0 1 4 4 1 0 4 3 6 6 6 4 0 1 1 0 5 0 2 2 3 6 6 0 0 6 0 1 6 1 0 0 0] tf.Tensor(\n",
      "[0 0 6 5 5 5 1 1 6 4 0 1 1 0 5 3 0 1 0 1 4 0 0 6 5 6 1 6 4 1 4 6 4 0 5 3 4\n",
      " 1 0 3 1 3 4 4 1 4 4 4 3 2 0 0 1 0 6 6 3 4 0 4 3 0 0 1 4 1 3 5 3 6 3 1 4 1\n",
      " 0 1 4 4 6 0 4 4 6 1 1 4 0 6 6 0 3 0 2 2 4 6 1 0 0 6 0 1 6 6 0 0 0], shape=(107,), dtype=int64)\n",
      "Accuracy: 0.7009345794392523\n"
     ]
    }
   ],
   "source": [
    "clf_a = svm.SVC(kernel='linear')\n",
    "clf_a.fit(Xa_train1, ya_train_emotions)\n",
    "ya_pred = clf_a.predict(Xa_test)\n",
    "print(yb_pred,yb_test_emotions)\n",
    "print(\"Accuracy:\",accuracy_score(ya_pred,ya_test_emotions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracies = [[0 for i in range(2)] for j in range(7)] \n",
    "\n",
    "svm_predict_angry_b= clf_b.predict(bef_angry_x)\n",
    "svm_accuracies[0][0]=round((accuracy_score(bef_angry_emotions,svm_predict_angry_b)),2)\n",
    "\n",
    "svm_predict_angry_a= clf_a.predict(aft_angry_x)\n",
    "svm_accuracies[0][1]=round(accuracy_score(aft_angry_emotions,svm_predict_angry_a),2)\n",
    "\n",
    "svm_predict_boredom_b= clf_b.predict(bef_boredom_x)\n",
    "svm_accuracies[1][0]=round(accuracy_score(bef_boredom_emotions,svm_predict_boredom_b),2)\n",
    "\n",
    "svm_predict_boredom_a= clf_a.predict(aft_boredom_x)\n",
    "svm_accuracies[1][1]=round(accuracy_score(aft_boredom_emotions,svm_predict_boredom_a),2)\n",
    "\n",
    "svm_predict_disgust_b= clf_b.predict(bef_disgust_x)\n",
    "svm_accuracies[2][0]=round(accuracy_score(bef_disgust_emotions,svm_predict_disgust_b),2)\n",
    "\n",
    "svm_predict_disgust_a= clf_a.predict(aft_disgust_x)\n",
    "svm_accuracies[2][1]=round(accuracy_score(aft_disgust_emotions,svm_predict_disgust_a),2)\n",
    "\n",
    "svm_predict_fear_b= clf_b.predict(bef_fear_x)\n",
    "svm_accuracies[3][0]=round(accuracy_score(bef_fear_emotions,svm_predict_fear_b),2)\n",
    "\n",
    "svm_predict_fear_a= clf_a.predict(aft_fear_x)\n",
    "svm_accuracies[3][1]=round(accuracy_score(aft_fear_emotions,svm_predict_fear_a),2)\n",
    "\n",
    "svm_predict_happy_b= clf_b.predict(bef_happy_x)\n",
    "svm_accuracies[4][0]=round(accuracy_score(bef_happy_emotions,svm_predict_happy_b),2)\n",
    "\n",
    "svm_predict_happy_a= clf_a.predict(aft_happy_x)\n",
    "svm_accuracies[4][1]=round(accuracy_score(aft_happy_emotions,svm_predict_happy_a),2)\n",
    "\n",
    "svm_predict_sad_b= clf_b.predict(bef_sad_x)\n",
    "svm_accuracies[5][0]=round(accuracy_score(bef_sad_emotions,svm_predict_sad_b),2)\n",
    "\n",
    "svm_predict_sad_a= clf_a.predict(aft_sad_x)\n",
    "svm_accuracies[5][1]=round(accuracy_score(aft_sad_emotions,svm_predict_sad_a),2)\n",
    "\n",
    "svm_predict_neutral_b= clf_b.predict(bef_neutral_x)\n",
    "svm_accuracies[6][0]=round(accuracy_score(bef_neutral_emotions,svm_predict_neutral_b),2)\n",
    "\n",
    "svm_predict_neutral_a= clf_a.predict(aft_neutral_x)\n",
    "svm_accuracies[6][1]=round(accuracy_score(aft_neutral_emotions,svm_predict_neutral_a),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The format is emotion[before_removal   after_removal]\n",
      "\n",
      "angry [0.98, 0.96]\n",
      "\n",
      "boredom [0.89, 0.93]\n",
      "\n",
      "disgust [0.98, 0.96]\n",
      "\n",
      "fear [0.91, 0.93]\n",
      "\n",
      "happy [0.89, 0.92]\n",
      "\n",
      "sad [1.0, 0.98]\n",
      "\n",
      "neutral [0.94, 0.91]\n"
     ]
    }
   ],
   "source": [
    "print(\"The format is emotion[before_removal   after_removal]\")\n",
    "print('\\nangry', (svm_accuracies[0]))\n",
    "print('\\nboredom', svm_accuracies[1])\n",
    "print('\\ndisgust', svm_accuracies[2])\n",
    "print('\\nfear', svm_accuracies[3])\n",
    "print('\\nhappy', svm_accuracies[4])\n",
    "print('\\nsad', svm_accuracies[5])\n",
    "print('\\nneutral', svm_accuracies[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the overall accuracy in predicting the emotions after the removal of silence is decreased in svm as dnn, the emotion wise accuracies are a bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now we didn't consider the speakers , to make this speaker independen,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two letters of the file name has the information of speaker as follows.\n",
    "\n",
    "This consists of 10 speakers with codes. \n",
    "\n",
    "03 - male, 31 years old\n",
    "\n",
    "08 - female, 34 years\n",
    "\n",
    "09 - female, 21 years\n",
    "\n",
    "10 - male, 32 years\n",
    "\n",
    "11 - male, 26 years\n",
    "\n",
    "12 - male, 30 years\n",
    "\n",
    "13 - female, 32 years\n",
    "\n",
    "14 - female, 35 years\n",
    "\n",
    "15 - male, 25 years\n",
    "\n",
    "16 - female, 31 years\n",
    "\n",
    "\n",
    "So, Let's divide them speaker wise to make our results speaker independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "M31_indices=[]\n",
    "F34_indices=[]\n",
    "F21_indices=[]\n",
    "M32_indices=[]\n",
    "M26_indices=[]\n",
    "M30_indices=[]\n",
    "F32_indices=[]\n",
    "F35_indices=[]\n",
    "M25_indices=[]\n",
    "F31_indices=[]\n",
    "for i in range(len(bef_rem)):\n",
    "    if(bef_rem[i][:2]=='03'):\n",
    "        M31_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='08'):\n",
    "        F34_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='09'):\n",
    "        F21_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='10'):\n",
    "        M32_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='11'):\n",
    "        M26_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='12'):\n",
    "        M30_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='13'):\n",
    "        F32_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='14'):\n",
    "        F35_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='15'):\n",
    "        M25_indices.append(i)\n",
    "    if(bef_rem[i][:2]=='16'):\n",
    "        F31_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_M31_x = np.zeros((len(M31_indices),mfcc_size))\n",
    "bef_M31_y = np.zeros((len(M31_indices),7),dtype=int)\n",
    "aft_M31_x = np.zeros((len(M31_indices),mfcc_size))\n",
    "aft_M31_y = np.zeros((len(M31_indices),7),dtype=int)\n",
    "for i in range(len(M31_indices)):\n",
    "    bef_M31_x[i] = np.resize(bef_x[M31_indices[i]],(1,mfcc_size))\n",
    "    bef_M31_y[i] = np.resize(bef_y[M31_indices[i]],(1,7))\n",
    "    aft_M31_x[i] = np.resize(aft_x[M31_indices[i]],(1,mfcc_size))\n",
    "    aft_M31_y[i] = np.resize(aft_y[M31_indices[i]],(1,7))\n",
    "\n",
    "bef_F34_x = np.zeros((len(F34_indices),mfcc_size))\n",
    "bef_F34_y = np.zeros((len(F34_indices),7),dtype=int)\n",
    "aft_F34_x = np.zeros((len(F34_indices),mfcc_size))\n",
    "aft_F34_y = np.zeros((len(F34_indices),7),dtype=int)\n",
    "for i in range(len(F34_indices)):\n",
    "    bef_F34_x[i] = np.resize(bef_x[F34_indices[i]],(1,mfcc_size))\n",
    "    bef_F34_y[i] = np.resize(bef_y[F34_indices[i]],(1,7))\n",
    "    aft_F34_x[i] = np.resize(aft_x[F34_indices[i]],(1,mfcc_size))\n",
    "    aft_F34_y[i] = np.resize(aft_y[F34_indices[i]],(1,7))\n",
    "\n",
    "bef_F21_x = np.zeros((len(F21_indices),mfcc_size))\n",
    "bef_F21_y = np.zeros((len(F21_indices),7),dtype=int)\n",
    "aft_F21_x = np.zeros((len(F21_indices),mfcc_size))\n",
    "aft_F21_y = np.zeros((len(F21_indices),7),dtype=int)\n",
    "for i in range(len(F21_indices)):\n",
    "    bef_F21_x[i] = np.resize(bef_x[F21_indices[i]],(1,mfcc_size))\n",
    "    bef_F21_y[i] = np.resize(bef_y[F21_indices[i]],(1,7))\n",
    "    aft_F21_x[i] = np.resize(aft_x[F21_indices[i]],(1,mfcc_size))\n",
    "    aft_F21_y[i] = np.resize(aft_y[F21_indices[i]],(1,7))\n",
    "\n",
    "bef_M32_x = np.zeros((len(M32_indices),mfcc_size))\n",
    "bef_M32_y = np.zeros((len(M32_indices),7),dtype=int)\n",
    "aft_M32_x = np.zeros((len(M32_indices),mfcc_size))\n",
    "aft_M32_y = np.zeros((len(M32_indices),7),dtype=int)\n",
    "for i in range(len(M32_indices)):\n",
    "    bef_M32_x[i] = np.resize(bef_x[M32_indices[i]],(1,mfcc_size))\n",
    "    bef_M32_y[i] = np.resize(bef_y[M32_indices[i]],(1,7))\n",
    "    aft_M32_x[i] = np.resize(aft_x[M32_indices[i]],(1,mfcc_size))\n",
    "    aft_M32_y[i] = np.resize(aft_y[M32_indices[i]],(1,7))\n",
    "\n",
    "bef_M26_x = np.zeros((len(M26_indices),mfcc_size))\n",
    "bef_M26_y = np.zeros((len(M26_indices),7),dtype=int)\n",
    "aft_M26_x = np.zeros((len(M26_indices),mfcc_size))\n",
    "aft_M26_y = np.zeros((len(M26_indices),7),dtype=int)\n",
    "for i in range(len(M26_indices)):\n",
    "    bef_M26_x[i] = np.resize(bef_x[M26_indices[i]],(1,mfcc_size))\n",
    "    bef_M26_y[i] = np.resize(bef_y[M26_indices[i]],(1,7))\n",
    "    aft_M26_x[i] = np.resize(aft_x[M26_indices[i]],(1,mfcc_size))\n",
    "    aft_M26_y[i] = np.resize(aft_y[M26_indices[i]],(1,7))\n",
    "\n",
    "bef_M30_x = np.zeros((len(M30_indices),mfcc_size))\n",
    "bef_M30_y = np.zeros((len(M30_indices),7),dtype=int)\n",
    "aft_M30_x = np.zeros((len(M30_indices),mfcc_size))\n",
    "aft_M30_y = np.zeros((len(M30_indices),7),dtype=int)\n",
    "for i in range(len(M30_indices)):\n",
    "    bef_M30_x[i] = np.resize(bef_x[M30_indices[i]],(1,mfcc_size))\n",
    "    bef_M30_y[i] = np.resize(bef_y[M30_indices[i]],(1,7))\n",
    "    aft_M30_x[i] = np.resize(aft_x[M30_indices[i]],(1,mfcc_size))\n",
    "    aft_M30_y[i] = np.resize(aft_y[M30_indices[i]],(1,7))\n",
    "\n",
    "bef_F32_x = np.zeros((len(F32_indices),mfcc_size))\n",
    "bef_F32_y = np.zeros((len(F32_indices),7),dtype=int)\n",
    "aft_F32_x = np.zeros((len(F32_indices),mfcc_size))\n",
    "aft_F32_y = np.zeros((len(F32_indices),7),dtype=int)\n",
    "for i in range(len(F32_indices)):\n",
    "    bef_F32_x[i] = np.resize(bef_x[F32_indices[i]],(1,mfcc_size))\n",
    "    bef_F32_y[i] = np.resize(bef_y[F32_indices[i]],(1,7))\n",
    "    aft_F32_x[i] = np.resize(aft_x[F32_indices[i]],(1,mfcc_size))\n",
    "    aft_F32_y[i] = np.resize(aft_y[F32_indices[i]],(1,7))\n",
    "    \n",
    "bef_F35_x = np.zeros((len(F35_indices),mfcc_size))\n",
    "bef_F35_y = np.zeros((len(F35_indices),7),dtype=int)\n",
    "aft_F35_x = np.zeros((len(F35_indices),mfcc_size))\n",
    "aft_F35_y = np.zeros((len(F35_indices),7),dtype=int)\n",
    "for i in range(len(F35_indices)):\n",
    "    bef_F35_x[i] = np.resize(bef_x[F35_indices[i]],(1,mfcc_size))\n",
    "    bef_F35_y[i] = np.resize(bef_y[F35_indices[i]],(1,7))\n",
    "    aft_F35_x[i] = np.resize(aft_x[F35_indices[i]],(1,mfcc_size))\n",
    "    aft_F35_y[i] = np.resize(aft_y[F35_indices[i]],(1,7))\n",
    "\n",
    "bef_M25_x = np.zeros((len(M25_indices),mfcc_size))\n",
    "bef_M25_y = np.zeros((len(M25_indices),7),dtype=int)\n",
    "aft_M25_x = np.zeros((len(M25_indices),mfcc_size))\n",
    "aft_M25_y = np.zeros((len(M25_indices),7),dtype=int)\n",
    "for i in range(len(M25_indices)):\n",
    "    bef_M25_x[i] = np.resize(bef_x[M25_indices[i]],(1,mfcc_size))\n",
    "    bef_M25_y[i] = np.resize(bef_y[M25_indices[i]],(1,7))\n",
    "    aft_M25_x[i] = np.resize(aft_x[M25_indices[i]],(1,mfcc_size))\n",
    "    aft_M25_y[i] = np.resize(aft_y[M25_indices[i]],(1,7))\n",
    "\n",
    "bef_F31_x = np.zeros((len(F31_indices),mfcc_size))\n",
    "bef_F31_y = np.zeros((len(F31_indices),7),dtype=int)\n",
    "aft_F31_x = np.zeros((len(F31_indices),mfcc_size))\n",
    "aft_F31_y = np.zeros((len(F31_indices),7),dtype=int)\n",
    "for i in range(len(F31_indices)):\n",
    "    bef_F31_x[i] = np.resize(bef_x[F31_indices[i]],(1,mfcc_size))\n",
    "    bef_F31_y[i] = np.resize(bef_y[F31_indices[i]],(1,7))\n",
    "    aft_F31_x[i] = np.resize(aft_x[F31_indices[i]],(1,mfcc_size))\n",
    "    aft_F31_y[i] = np.resize(aft_y[F31_indices[i]],(1,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
